{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "snowStemmer = SnowballStemmer(language='german')\n",
    "RAW_DATA_PATH = 'data/raw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def find_longest_word(word_list):\n",
    "    longest_word =  max(word_list, key=len)\n",
    "    return len(longest_word)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def lemmatize_string(input_text):\n",
    "    doc = nlp(input_text.lower())\n",
    "    result = ' '.join([x.lemma_ for x in doc])\n",
    "    doc = nlp(result.title())\n",
    "    result = ' '.join([x.lemma_ for x in doc]).upper()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def stemm_List_of_Words(input_text):\n",
    "    result = [snowStemmer.stem(word).upper() for word in input_text]\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def contains_song(ID, JAHR, MONAT):\n",
    "    return  len(df.loc[(df['ID'] == ID) & (df['JAHR'] == JAHR) & (df['MONAT'] == MONAT)]) >= 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def isSeasonal():\n",
    "    for index, row in df.iterrows():\n",
    "        if np.isnan(row['IS_SEASONAL']) or False:\n",
    "            df.loc[(df['ID'] == row['ID']), 'IS_SEASONAL'] = contains_song(row['ID'], row['JAHR'] + 1, row['MONAT'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def getAnteil(word_list, text_len):\n",
    "    l = [round(word_list.count(word)/text_len, 4) for word in word_list]\n",
    "    return mean(l)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_nr_Sublist(l, s):\n",
    "    nr = 0\n",
    "    empty_sublist = False\n",
    "    if s == []:\n",
    "        empty_sublist = True\n",
    "    if not empty_sublist:\n",
    "        for i in range(len(l)):\n",
    "            if l[i] == s[0]:\n",
    "                n = 1\n",
    "                while (n < len(s)) and i+n < len(l) and (l[i+n] == s[n]):\n",
    "                    n += 1\n",
    "                if n == len(s):\n",
    "                    nr+=1\n",
    "    return nr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df_Lied = pd.read_csv(RAW_DATA_PATH + 'LIED.csv', usecols=['ID','INTERPRET', 'TITEL', 'SPRACHE_DEUTSCH', 'TEXT_TEIL1', 'TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4'])\n",
    "#print(df_Lied.head())\n",
    "\n",
    "df_Chart_Position = pd.read_csv(RAW_DATA_PATH + 'CHART_POSITION.csv', usecols=['LIED_ID', 'POSITION', 'DATUM_VON', 'DATUM_BIS'])\n",
    "#print(df_Chart_Position.head())\n",
    "\n",
    "#get stopword-list\n",
    "with open(RAW_DATA_PATH+'Stoppwords.csv', newline='', encoding='UTF-8') as f:\n",
    "    stopwords_list = list(csv.reader(f))\n",
    "stopwords_list = [word.upper() for word in flatten(stopwords_list)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Conversion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df_Lied['TEXT'] = df_Lied['TEXT_TEIL1'].fillna('') + df_Lied['TEXT_TEIL2'].fillna('') + df_Lied['TEXT_TEIL3'].fillna('') + df_Lied['TEXT_TEIL4'].fillna('')\n",
    "\n",
    "df_Chart_Position['DATUM_VON'] = pd.to_datetime(df_Chart_Position['DATUM_VON'])\n",
    "df_Chart_Position['DATUM_BIS'] = pd.to_datetime(df_Chart_Position['DATUM_BIS'])\n",
    "df_Chart_Position['DAUER'] = (df_Chart_Position['DATUM_BIS'] - df_Chart_Position['DATUM_VON']).dt.days.astype('int16')\n",
    "df_Chart_Position['JAHR'] = df_Chart_Position['DATUM_BIS'].dt.year.astype('int16')\n",
    "df_Chart_Position['MONAT'] =  df_Chart_Position['DATUM_BIS'].dt.month.astype('int16')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df_Lied['processed_TEXT'] = df_Lied['TEXT']\n",
    "\n",
    "#lemmatization\n",
    "#df_Lied['processed_TEXT'] = df_Lied.processed_TEXT.apply(lambda text: lemmatize_string(text))\n",
    "\n",
    "# tokenize\n",
    "df_Lied['processed_TEXT'] = df_Lied.processed_TEXT.apply(lambda text: nltk.word_tokenize(text))\n",
    "\n",
    "#Stemming\n",
    "df_Lied['processed_TEXT'] = df_Lied.processed_TEXT.apply(lambda text: stemm_List_of_Words(text))\n",
    "\n",
    "#remove stopwords\n",
    "df_Lied['processed_TEXT'] = df_Lied.processed_TEXT.apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "\n",
    "#remove numbers\n",
    "df_Lied['processed_TEXT'] = df_Lied.processed_TEXT.apply(lambda word_list : [re.sub('\\w*\\d\\w*','NUM', word) for word in word_list])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LASS', 'STRAND', 'WEISST', 'WELCH', 'BRAUCH', 'MAL', 'WIED', 'MEER', 'SAND', 'YEAH', 'BRAUCH', 'MAL', 'WIEDERGRUND', 'BLEIB', 'AI', 'UHR', 'FAELLT', 'BLAU', 'SEH', 'VERSINKT', 'SETZT', 'LANGSAM', 'WASS', 'SPIEGELGLATT', 'BLINKT', 'WOLL', 'SEHNSUCHT', 'MORG', 'HEUT', 'PERFEKT', 'IS', 'SEHNSUCHT', 'MORG', 'HEUT', 'PERFEKT', 'IS', 'STEH', 'NEB', 'ZEIG', 'ECHT', 'LICHT', 'STADT', 'VERBRANNT', 'LASS', 'STRAND', 'KOMM', 'STEIG', 'LEUCHT', 'DIAMANT', 'YEH', 'ALL', 'SALZ', 'HAUT', 'AI', 'SEH', 'GESTOCH', 'SCHARF', 'TUT', 'FAST', 'AUG', 'WEH', 'ENDLICH', 'NOCH', 'NIE', 'GESEH', 'YEHI', 'WOLL', 'SEHNSUCHT', 'MORG', 'HEUT', 'PERFEKT', 'IS', 'SEHNSUCHT', 'MORG', 'HEUT', 'PERFEKT', 'IS', 'STEH', 'NEB', 'ZEIG', 'ECHT', 'LICHT', 'STADT', 'VERBRANNT', 'LASS', 'STRAND', 'LASS', 'STRAND', 'WEISST', 'WELCH', 'LASS', 'STRAND', 'YEAH', 'BRAUCHGRUND', 'BLEIB', 'AI', 'LASS', 'STRAND', 'WEISST', 'WELCH', 'BRAUCH', 'MAL', 'WIED', 'MEER', 'SAND', 'YEAH', 'BRAUCH', 'MAL', 'WIEDERGRUND', 'BLEIB', 'OUH', 'LASS', 'STRAND']\n"
     ]
    }
   ],
   "source": [
    "print(df_Lied['processed_TEXT'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN ICH BRAUCH MAL WIEDER MEER UND SAND  YEAH BRAUCH MAL WIEDERGRUND ZU BLEIBEN  AI DEINE UHR FAELLT INS BLAU WIR SEHEN  WIE DIE ZEIT VERSINKT WIR SETZTEN LANGSAM AUF DAS WASSER SPIEGELGLATT UND BLINKT  WIR WOLLEN DIE SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS DU STEHST NEBEN MIR  ZEIGST MIR  DASS ES ECHT IST DIE LICHTER MEINER STADT SIND VERBRANNT LASS UNS ZUM STRAND    KOMM WIR STEIGEN AUS DU LEUCHTEST WIE AUS DIAMANT  YEHE MIT ALL DEM SALZ AUF DER HAUT  AI ICH SEH DICH GESTOCHEN SCHARF ES TUT FAST DEN AUGEN WEH UND ENDLICH BIST DU DA SO HABE ICH DICH NOCH NIE GESEHEN  YEHI  WIR WOLLEN DIE SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS DU STEHST NEBEN MIR  ZEIGST MIR  DASS ES ECHT IST DIE LICHTER MEINER STADT SIND VERBRANNT LASS UNS ZUM STRAND   LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN LASS UNS ZUM STRAND  YEAH ICH BRAUCHGRUND ZU BLEIBEN  AI LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN ICH BRAUCH MAL WIEDER MEER UND SAND  YEAH BRAUCH MAL WIEDERGRUND ZU BLEIBEN  OUH LASS UNS ZUM STRAND \n"
     ]
    }
   ],
   "source": [
    "print(df_Lied['TEXT'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID       INTERPRET   TITEL  \\\n",
      "7219  408     Rumpelstilz  Kiosk    \n",
      "7220  408     Rumpelstilz  Kiosk    \n",
      "7763  425  Costa Cordalis  Anita    \n",
      "7762  425  Costa Cordalis  Anita    \n",
      "7761  425  Costa Cordalis  Anita    \n",
      "\n",
      "                                                   TEXT  \\\n",
      "7219  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "7220  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "7763  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "7762  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "7761  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "\n",
      "                                         processed_TEXT  POSITION  DATUM_VON  \\\n",
      "7219  [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...        50 2009-03-06   \n",
      "7220  [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...         4 1984-10-22   \n",
      "7763  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        23 2003-08-25   \n",
      "7762  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        39 2003-07-28   \n",
      "7761  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        27 2003-07-28   \n",
      "\n",
      "      DATUM_BIS  DAUER  JAHR  MONAT  \n",
      "7219 2009-03-12      6  2009      3  \n",
      "7220 1984-10-28      6  1984     10  \n",
      "7763 2003-08-31      6  2003      8  \n",
      "7762 2003-08-03      6  2003      8  \n",
      "7761 2003-08-03      6  2003      8  \n"
     ]
    }
   ],
   "source": [
    "df_Lied.drop(['TEXT_TEIL1','TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4', 'SPRACHE_DEUTSCH'], axis=1, inplace=True)\n",
    "df_Date = df_Chart_Position[['LIED_ID', 'DATUM_VON', 'DATUM_BIS', 'DAUER','JAHR', 'MONAT']]\n",
    "df_Lied.sort_values(by='ID', inplace=True)\n",
    "df_Chart_Position.sort_values(by='LIED_ID', inplace=True)\n",
    "df = pd.concat([df_Lied, df_Chart_Position], axis='columns')\n",
    "df.drop('LIED_ID', axis=1, inplace=True)\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature Creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "df['ANZ_UNIQUE_WOERTER'] = list(len(set(word)) for word in df['processed_TEXT'])\n",
    "df['MAX_WORT_WDH'] = [max(Counter(text).values()) for text in df['processed_TEXT']]\n",
    "df['LEN_TEXT'] = [len(text) for text in df['processed_TEXT']]\n",
    "df['PCT_WORT_WDH'] = [getAnteil(word_list, len(word_list)) for word_list in df['processed_TEXT']]\n",
    "\n",
    "max_word_list = []\n",
    "for text in df['processed_TEXT']:\n",
    "    count = dict(Counter(text).items())\n",
    "    count = {k: v for k, v in sorted(count.items(), key=lambda item: item[1], reverse=True)}\n",
    "    key_list = [key for key in count.keys()]\n",
    "    values_list = [key for key in count.values()]\n",
    "    text_dict = {'WORD': key_list, 'FREQ': values_list}\n",
    "    max_word = key_list[0]\n",
    "    max_word_list.append(max_word)\n",
    "df['WORT_MAX_WDH'] = max_word_list\n",
    "\n",
    "df['LEN_LAENGSTES_WORT'] = list(len(max(set(word), key=len)) for word in df['processed_TEXT'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "            ID                    INTERPRET           TITEL  \\\n7219       408                  Rumpelstilz          Kiosk    \n7220       408                  Rumpelstilz          Kiosk    \n7763       425               Costa Cordalis          Anita    \n7762       425               Costa Cordalis          Anita    \n7761       425               Costa Cordalis          Anita    \n...        ...                          ...             ...   \n22129  2303823            AnnenMayKantereit  3 Tage am Meer   \n22014  2304169          Farid Bang & B-Case          Baller   \n22651  2304914                    Yung Hurn         Alleine   \n21906  2304939                  Azet & Zuna          Ohh oh   \n22150  2305285  Capital Bra & Lucry & Suena           Musik   \n\n                                                    TEXT  \\\n7219   ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n7220   ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n7763   JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n7762   JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n7761   JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n...                                                  ...   \n22129  ICH WEISS  WAS ICH KANN TROTZDEM AB UND AN KOM...   \n22014  BALLER   LA LA LA  ICH ZIEH  DIE WAFFE UND ICH...   \n22651  BABY  SAG MIR  WAS DU HEUTE MACHST HAST DU ZEI...   \n21906  OH  OH  AH OH   ULTRA PLUS  BABY     BIN MEIN ...   \n22150  BITTE BITTE GIB  MIR DAS GEFUEHL  DASS DU MICH...   \n\n                                          processed_TEXT  POSITION  DATUM_VON  \\\n7219   [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...        50 2009-03-06   \n7220   [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...         4 1984-10-22   \n7763   [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        23 2003-08-25   \n7762   [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        39 2003-07-28   \n7761   [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        27 2003-07-28   \n...                                                  ...       ...        ...   \n22129  [WEISS, KANN, TROTZD, KOMM, EN, ZWEIFEL, DARAN...        21 1980-11-24   \n22014  [BALL, LA, LA, LA, ZIEH, WAFF, BALL, LA, LA, L...        42 2020-04-10   \n22651  [BABY, SAG, HEUT, MACH, JA, BABY, SAG, HEUT, M...        20 2019-10-18   \n21906  [OH, OH, AH, OH, ULTRA, BABY, EIG, CHEF, SITZ,...         5 2019-07-26   \n22150  [BITT, BITT, GIB, GEFUEHL, NOCH, LIEB, TAG, WA...        38 2020-04-24   \n\n       DATUM_BIS  DAUER  JAHR  MONAT  ANZ_UNIQUE_WOERTER  MAX_WORT_WDH  \\\n7219  2009-03-12      6  2009      3                  83             6   \n7220  1984-10-28      6  1984     10                  83             6   \n7763  2003-08-31      6  2003      8                  73            16   \n7762  2003-08-03      6  2003      8                  73            16   \n7761  2003-08-03      6  2003      8                  73            16   \n...          ...    ...   ...    ...                 ...           ...   \n22129 1980-11-30      6  1980     11                  45            11   \n22014 2020-04-16      6  2020      4                 149            42   \n22651 2019-10-24      6  2019     10                  58            28   \n21906 2019-08-01      6  2019      8                 109            67   \n22150 2020-04-30      6  2020      4                 117            31   \n\n       LEN_TEXT  PCT_WORT_WDH WORT_MAX_WDH  LEN_LAENGSTES_WORT  \n7219        106      0.016005        KIOSK                  13  \n7220        106      0.016005        KIOSK                  13  \n7763        141      0.028338        ANITA                  12  \n7762        141      0.028338        ANITA                  12  \n7761        141      0.028338        ANITA                  12  \n...         ...           ...          ...                 ...  \n22129       121      0.054844         DREI                  11  \n22014       327      0.032717           LA                  14  \n22651       173      0.066797           JA                   7  \n21906       240      0.087820           OH                  13  \n22150       247      0.029594         BITT                  16  \n\n[22762 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>INTERPRET</th>\n      <th>TITEL</th>\n      <th>TEXT</th>\n      <th>processed_TEXT</th>\n      <th>POSITION</th>\n      <th>DATUM_VON</th>\n      <th>DATUM_BIS</th>\n      <th>DAUER</th>\n      <th>JAHR</th>\n      <th>MONAT</th>\n      <th>ANZ_UNIQUE_WOERTER</th>\n      <th>MAX_WORT_WDH</th>\n      <th>LEN_TEXT</th>\n      <th>PCT_WORT_WDH</th>\n      <th>WORT_MAX_WDH</th>\n      <th>LEN_LAENGSTES_WORT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7219</th>\n      <td>408</td>\n      <td>Rumpelstilz</td>\n      <td>Kiosk</td>\n      <td>ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...</td>\n      <td>[ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...</td>\n      <td>50</td>\n      <td>2009-03-06</td>\n      <td>2009-03-12</td>\n      <td>6</td>\n      <td>2009</td>\n      <td>3</td>\n      <td>83</td>\n      <td>6</td>\n      <td>106</td>\n      <td>0.016005</td>\n      <td>KIOSK</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>7220</th>\n      <td>408</td>\n      <td>Rumpelstilz</td>\n      <td>Kiosk</td>\n      <td>ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...</td>\n      <td>[ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...</td>\n      <td>4</td>\n      <td>1984-10-22</td>\n      <td>1984-10-28</td>\n      <td>6</td>\n      <td>1984</td>\n      <td>10</td>\n      <td>83</td>\n      <td>6</td>\n      <td>106</td>\n      <td>0.016005</td>\n      <td>KIOSK</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>7763</th>\n      <td>425</td>\n      <td>Costa Cordalis</td>\n      <td>Anita</td>\n      <td>JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...</td>\n      <td>[JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...</td>\n      <td>23</td>\n      <td>2003-08-25</td>\n      <td>2003-08-31</td>\n      <td>6</td>\n      <td>2003</td>\n      <td>8</td>\n      <td>73</td>\n      <td>16</td>\n      <td>141</td>\n      <td>0.028338</td>\n      <td>ANITA</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>7762</th>\n      <td>425</td>\n      <td>Costa Cordalis</td>\n      <td>Anita</td>\n      <td>JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...</td>\n      <td>[JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...</td>\n      <td>39</td>\n      <td>2003-07-28</td>\n      <td>2003-08-03</td>\n      <td>6</td>\n      <td>2003</td>\n      <td>8</td>\n      <td>73</td>\n      <td>16</td>\n      <td>141</td>\n      <td>0.028338</td>\n      <td>ANITA</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>7761</th>\n      <td>425</td>\n      <td>Costa Cordalis</td>\n      <td>Anita</td>\n      <td>JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...</td>\n      <td>[JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...</td>\n      <td>27</td>\n      <td>2003-07-28</td>\n      <td>2003-08-03</td>\n      <td>6</td>\n      <td>2003</td>\n      <td>8</td>\n      <td>73</td>\n      <td>16</td>\n      <td>141</td>\n      <td>0.028338</td>\n      <td>ANITA</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22129</th>\n      <td>2303823</td>\n      <td>AnnenMayKantereit</td>\n      <td>3 Tage am Meer</td>\n      <td>ICH WEISS  WAS ICH KANN TROTZDEM AB UND AN KOM...</td>\n      <td>[WEISS, KANN, TROTZD, KOMM, EN, ZWEIFEL, DARAN...</td>\n      <td>21</td>\n      <td>1980-11-24</td>\n      <td>1980-11-30</td>\n      <td>6</td>\n      <td>1980</td>\n      <td>11</td>\n      <td>45</td>\n      <td>11</td>\n      <td>121</td>\n      <td>0.054844</td>\n      <td>DREI</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>22014</th>\n      <td>2304169</td>\n      <td>Farid Bang &amp; B-Case</td>\n      <td>Baller</td>\n      <td>BALLER   LA LA LA  ICH ZIEH  DIE WAFFE UND ICH...</td>\n      <td>[BALL, LA, LA, LA, ZIEH, WAFF, BALL, LA, LA, L...</td>\n      <td>42</td>\n      <td>2020-04-10</td>\n      <td>2020-04-16</td>\n      <td>6</td>\n      <td>2020</td>\n      <td>4</td>\n      <td>149</td>\n      <td>42</td>\n      <td>327</td>\n      <td>0.032717</td>\n      <td>LA</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>22651</th>\n      <td>2304914</td>\n      <td>Yung Hurn</td>\n      <td>Alleine</td>\n      <td>BABY  SAG MIR  WAS DU HEUTE MACHST HAST DU ZEI...</td>\n      <td>[BABY, SAG, HEUT, MACH, JA, BABY, SAG, HEUT, M...</td>\n      <td>20</td>\n      <td>2019-10-18</td>\n      <td>2019-10-24</td>\n      <td>6</td>\n      <td>2019</td>\n      <td>10</td>\n      <td>58</td>\n      <td>28</td>\n      <td>173</td>\n      <td>0.066797</td>\n      <td>JA</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>21906</th>\n      <td>2304939</td>\n      <td>Azet &amp; Zuna</td>\n      <td>Ohh oh</td>\n      <td>OH  OH  AH OH   ULTRA PLUS  BABY     BIN MEIN ...</td>\n      <td>[OH, OH, AH, OH, ULTRA, BABY, EIG, CHEF, SITZ,...</td>\n      <td>5</td>\n      <td>2019-07-26</td>\n      <td>2019-08-01</td>\n      <td>6</td>\n      <td>2019</td>\n      <td>8</td>\n      <td>109</td>\n      <td>67</td>\n      <td>240</td>\n      <td>0.087820</td>\n      <td>OH</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>22150</th>\n      <td>2305285</td>\n      <td>Capital Bra &amp; Lucry &amp; Suena</td>\n      <td>Musik</td>\n      <td>BITTE BITTE GIB  MIR DAS GEFUEHL  DASS DU MICH...</td>\n      <td>[BITT, BITT, GIB, GEFUEHL, NOCH, LIEB, TAG, WA...</td>\n      <td>38</td>\n      <td>2020-04-24</td>\n      <td>2020-04-30</td>\n      <td>6</td>\n      <td>2020</td>\n      <td>4</td>\n      <td>117</td>\n      <td>31</td>\n      <td>247</td>\n      <td>0.029594</td>\n      <td>BITT</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n<p>22762 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RANK_SCORES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "MAX_RANK = 50\n",
    "df['RANK_SCORE'] = MAX_RANK - df['POSITION'] + 1\n",
    "df['MAX_RANK_SCORE'] = [max(df.loc[df.ID == id_, 'RANK_SCORE']) for id_ in df['ID']]\n",
    "df['MEAN_RANK_SCORE'] = [round(mean(df.loc[df.ID == id_, 'RANK_SCORE'])) for id_ in df['ID']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Percentage of Stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "df['NUMBER_OF_STOPWORDS'] = df.TEXT.str.split().apply(lambda x: len(set(x) & set(stopwords_list)))\n",
    "df['PCT_STOPWORD'] = df.NUMBER_OF_STOPWORDS.apply(lambda row: round(row/len(df['TEXT']), ndigits=5))\n",
    "df.drop('NUMBER_OF_STOPWORDS', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID       INTERPRET   TITEL  \\\n",
      "0  408     Rumpelstilz  Kiosk    \n",
      "1  408     Rumpelstilz  Kiosk    \n",
      "2  425  Costa Cordalis  Anita    \n",
      "3  425  Costa Cordalis  Anita    \n",
      "4  425  Costa Cordalis  Anita    \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "1  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "2  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "3  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "4  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "\n",
      "                                      processed_TEXT  POSITION  DATUM_VON  \\\n",
      "0  [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...        50 2009-03-06   \n",
      "1  [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...         4 1984-10-22   \n",
      "2  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        23 2003-08-25   \n",
      "3  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        39 2003-07-28   \n",
      "4  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        27 2003-07-28   \n",
      "\n",
      "   DATUM_BIS  DAUER  JAHR  ...  ANZ_UNIQUE_WOERTER  MAX_WORT_WDH  LEN_TEXT  \\\n",
      "0 2009-03-12      6  2009  ...                  83             6       106   \n",
      "1 1984-10-28      6  1984  ...                  83             6       106   \n",
      "2 2003-08-31      6  2003  ...                  73            16       141   \n",
      "3 2003-08-03      6  2003  ...                  73            16       141   \n",
      "4 2003-08-03      6  2003  ...                  73            16       141   \n",
      "\n",
      "   PCT_WORT_WDH  WORT_MAX_WDH LEN_LAENGSTES_WORT  RANK_SCORE  MAX_RANK_SCORE  \\\n",
      "0      0.016005         KIOSK                 13           1              47   \n",
      "1      0.016005         KIOSK                 13          47              47   \n",
      "2      0.028338         ANITA                 12          28              49   \n",
      "3      0.028338         ANITA                 12          12              49   \n",
      "4      0.028338         ANITA                 12          24              49   \n",
      "\n",
      "   MEAN_RANK_SCORE  PCT_STOPWORD  \n",
      "0               24       0.00211  \n",
      "1               24       0.00211  \n",
      "2               27       0.00189  \n",
      "3               27       0.00189  \n",
      "4               27       0.00189  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.drop('index',axis=1, inplace=True)\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Title Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "#lemmatize title\n",
    "#df['processed_TITLE'] = df.TITEL.apply(lambda titel: ' '.join([x.lemma_ for x in nlp(titel)]))\n",
    "#print(df['processed_TITLE'].head())\n",
    "\n",
    "#tokenize title\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['processed_TITLE'] = df.TITEL.apply(lambda titel: [word for word in tokenizer.tokenize(titel)])\n",
    "\n",
    "#Stemming\n",
    "df['processed_TITLE'] = df.processed_TITLE.apply(lambda titel: stemm_List_of_Words(titel))\n",
    "\n",
    "#remove stopwords\n",
    "df['processed_TITLE'] = df.processed_TITLE.apply(lambda titel: [word for word in titel if word.upper() not in stopwords_list])\n",
    "\n",
    "#remove numbers\n",
    "df['processed_TITLE'] = df.processed_TITLE.apply(lambda word_list : [re.sub('\\w*\\d\\w*','NUM', word) for word in word_list])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "df['LEN_TITLE'] = [len(title_list) for title_list in df['processed_TITLE']]\n",
    "df['ANZ_TITLE_WDH'] = [get_nr_Sublist(df.processed_TEXT[i], df.processed_TITLE[i]) for i in df.index]\n",
    "df['PCT_TITLE_WDH'] = [df.ANZ_TITLE_WDH[i]/round(df.LEN_TEXT[i]/df.LEN_TITLE[i]) if (df.LEN_TITLE[i] != 0) else 0 for i in df.index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID       INTERPRET   TITEL  \\\n",
      "0  408     Rumpelstilz  Kiosk    \n",
      "1  408     Rumpelstilz  Kiosk    \n",
      "2  425  Costa Cordalis  Anita    \n",
      "3  425  Costa Cordalis  Anita    \n",
      "4  425  Costa Cordalis  Anita    \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "1  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "2  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "3  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "4  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "\n",
      "                                      processed_TEXT  POSITION  DATUM_VON  \\\n",
      "0  [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...        50 2009-03-06   \n",
      "1  [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...         4 1984-10-22   \n",
      "2  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        23 2003-08-25   \n",
      "3  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        39 2003-07-28   \n",
      "4  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        27 2003-07-28   \n",
      "\n",
      "   DATUM_BIS  DAUER  JAHR  ...  WORT_MAX_WDH  LEN_LAENGSTES_WORT  RANK_SCORE  \\\n",
      "0 2009-03-12      6  2009  ...         KIOSK                  13           1   \n",
      "1 1984-10-28      6  1984  ...         KIOSK                  13          47   \n",
      "2 2003-08-31      6  2003  ...         ANITA                  12          28   \n",
      "3 2003-08-03      6  2003  ...         ANITA                  12          12   \n",
      "4 2003-08-03      6  2003  ...         ANITA                  12          24   \n",
      "\n",
      "   MAX_RANK_SCORE  MEAN_RANK_SCORE PCT_STOPWORD  processed_TITLE  LEN_TITLE  \\\n",
      "0              47               24      0.00211          [KIOSK]          1   \n",
      "1              47               24      0.00211          [KIOSK]          1   \n",
      "2              49               27      0.00189          [ANITA]          1   \n",
      "3              49               27      0.00189          [ANITA]          1   \n",
      "4              49               27      0.00189          [ANITA]          1   \n",
      "\n",
      "   ANZ_TITLE_WDH  PCT_TITLE_WDH  \n",
      "0              6       0.056604  \n",
      "1              6       0.056604  \n",
      "2             16       0.113475  \n",
      "3             16       0.113475  \n",
      "4             16       0.113475  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    WORD  FREQ\n",
      "0    NUM  1438\n",
      "1   LIEB   988\n",
      "2    ALL   452\n",
      "3  NACHT   417\n",
      "4    NUR   375\n"
     ]
    }
   ],
   "source": [
    "title_list = [title for title in df['processed_TITLE']]\n",
    "count = dict(Counter(flatten(title_list)).items())\n",
    "count = {k: v for k, v in sorted(count.items(), key=lambda item: item[1], reverse=True)}\n",
    "key_list = [key for key in count.keys()]\n",
    "values_list = [key for key in count.values()]\n",
    "title_dict = {'WORD': key_list, 'FREQ': values_list}\n",
    "df_TITEL = pd.DataFrame(title_dict)\n",
    "print(df_TITEL.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WORD   FREQ\n",
      "0   ALL  59080\n",
      "1  DOCH  49080\n",
      "2   NUR  45266\n",
      "3  FUER  41810\n",
      "4    JA  39032\n"
     ]
    }
   ],
   "source": [
    "text_list = [text for text in df['processed_TEXT']]\n",
    "count = dict(Counter(flatten(text_list)).items())\n",
    "count = {k: v for k, v in sorted(count.items(), key=lambda item: item[1], reverse=True)}\n",
    "key_list = [key for key in count.keys()]\n",
    "values_list = [key for key in count.values()]\n",
    "text_dict = {'WORD': key_list, 'FREQ': values_list}\n",
    "df_TEXT = pd.DataFrame(text_dict)\n",
    "print(df_TEXT.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonal determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0        False\n1        False\n2        False\n3        False\n4        False\n         ...  \n22757    False\n22758    False\n22759    False\n22760    False\n22761    False\nName: IS_SEASONAL, Length: 22762, dtype: object"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['IS_SEASONAL'] = np.nan\n",
    "isSeasonal()\n",
    "df['IS_SEASONAL']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "MULTILINGUAL DETECTION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "df['IS_MULTILINGUAL'] = [False for _ in range(0, len(df))]\n",
    "df['PCT_GERMAN'] = np.nan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22762, 28)\n"
     ]
    }
   ],
   "source": [
    "new_cols = ['ID', 'INTERPRET', 'TITEL', 'processed_TITLE', 'TEXT', 'processed_TEXT', 'DATUM_VON', 'DATUM_BIS', 'JAHR', 'MONAT', 'DAUER','ANZ_UNIQUE_WOERTER', 'MAX_WORT_WDH', 'WORT_MAX_WDH','LEN_LAENGSTES_WORT', 'ANZ_TITLE_WDH', 'PCT_STOPWORD','PCT_WORT_WDH', 'PCT_TITLE_WDH', 'LEN_TITLE','LEN_TEXT', 'IS_SEASONAL', 'IS_MULTILINGUAL','PCT_GERMAN', 'POSITION', 'RANK_SCORE', 'MAX_RANK_SCORE', 'MEAN_RANK_SCORE']\n",
    "df=df[new_cols]\n",
    "df=df.reindex(columns=new_cols)\n",
    "print(np.shape(df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['ID', 'INTERPRET', 'TITEL', 'processed_TITLE', 'TEXT', 'processed_TEXT',\n       'DATUM_VON', 'DATUM_BIS', 'JAHR', 'MONAT', 'DAUER',\n       'ANZ_UNIQUE_WOERTER', 'MAX_WORT_WDH', 'WORT_MAX_WDH',\n       'LEN_LAENGSTES_WORT', 'ANZ_TITLE_WDH', 'PCT_STOPWORD', 'PCT_WORT_WDH',\n       'PCT_TITLE_WDH', 'LEN_TITLE', 'LEN_TEXT', 'IS_SEASONAL',\n       'IS_MULTILINGUAL', 'PCT_GERMAN', 'POSITION', 'RANK_SCORE',\n       'MAX_RANK_SCORE', 'MEAN_RANK_SCORE'],\n      dtype='object')"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data/processed/EDA.csv')\n",
    "#df_Date.to_csv('Data/processed/DATE.csv')\n",
    "df_TITEL.to_csv('Data/processed/TITLE-ANALYSIS.csv')\n",
    "df_TEXT.to_csv('Data/processed/TEXT-ANALYSIS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import io\n",
    "engine = create_engine('postgresql://OutsideUser:Xpr9XyDwx3ZfJyq2BNaW@92.205.167.58:8787/text_analytics')\n",
    "df.head(0).to_sql('processed_data', engine, if_exists='replace',index=False) #drops old table and creates new empty table\n",
    "conn = engine.raw_connection()\n",
    "cur = conn.cursor()\n",
    "output = io.StringIO()\n",
    "df.to_csv(output, sep='\\t', header=False, index=False)\n",
    "output.seek(0)\n",
    "contents = output.getvalue()\n",
    "cur.copy_from(output, 'processed_data', null=\"\") # null values become ''\n",
    "conn.commit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
