{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "snowStemmer = SnowballStemmer(language='german')\n",
    "RAW_DATA_PATH = 'data/raw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def find_longest_word(word_list):\n",
    "    longest_word =  max(word_list, key=len)\n",
    "    return len(longest_word)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def lemmatize_string(input_text):\n",
    "    doc = nlp(input_text.lower())\n",
    "    result = ' '.join([x.lemma_ for x in doc])\n",
    "    doc = nlp(result.title())\n",
    "    result = ' '.join([x.lemma_ for x in doc]).upper()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def stemm_List_of_Words(input_text):\n",
    "    result = [snowStemmer.stem(word).upper() for word in input_text]\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def contains_song(ID, JAHR, MONAT):\n",
    "    return  len(df.loc[(df['ID'] == ID) & (df['JAHR'] == JAHR) & (df['MONAT'] == MONAT)]) >= 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def isSeasonal():\n",
    "    for index, row in df.iterrows():\n",
    "        if np.isnan(row['SEASONAL?']) or False:\n",
    "            df.loc[(df['ID'] == row['ID']), 'SEASONAL?'] = contains_song(row['ID'], row['JAHR'] + 1, row['MONAT'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Lied = pd.read_csv(RAW_DATA_PATH + 'LIED.csv', usecols=['ID','INTERPRET', 'TITEL', 'SPRACHE_DEUTSCH', 'TEXT_TEIL1', 'TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4'])\n",
    "#print(df_Lied.head())\n",
    "\n",
    "df_Chart_Position = pd.read_csv(RAW_DATA_PATH + 'CHART_POSITION.csv', usecols=['LIED_ID', 'POSITION', 'DATUM_VON', 'DATUM_BIS'])\n",
    "#print(df_Chart_Position.head())\n",
    "\n",
    "#get stopword-list\n",
    "with open(RAW_DATA_PATH+'Stoppwords.csv', newline='', encoding='UTF-8') as f:\n",
    "    stopwords_list = list(csv.reader(f))\n",
    "stopwords_list = [word.upper() for word in flatten(stopwords_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Lied['TEXT'] = df_Lied['TEXT_TEIL1'].fillna('') + df_Lied['TEXT_TEIL2'].fillna('') + df_Lied['TEXT_TEIL3'].fillna('') + df_Lied['TEXT_TEIL4'].fillna('')\n",
    "\n",
    "df_Chart_Position['DATUM_VON'] = pd.to_datetime(df_Chart_Position['DATUM_VON'])\n",
    "df_Chart_Position['DATUM_BIS'] = pd.to_datetime(df_Chart_Position['DATUM_BIS'])\n",
    "df_Chart_Position['DAUER'] = (df_Chart_Position['DATUM_BIS'] - df_Chart_Position['DATUM_VON']).dt.days.astype('int16')\n",
    "df_Chart_Position['JAHR'] = df_Chart_Position['DATUM_BIS'].dt.year.astype('int16')\n",
    "df_Chart_Position['MONAT'] =  df_Chart_Position['DATUM_BIS'].dt.month.astype('int16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Lied['processed_TEXT'] = df_Lied['TEXT']\n",
    "\n",
    "#lemmatization\n",
    "#df_Lied['processed_TEXT'] = df_Lied.processed_TEXT.apply(lambda text: lemmatize_string(text))\n",
    "\n",
    "# tokenize\n",
    "df_Lied['processed_TEXT'] = df_Lied.processed_TEXT.apply(lambda text: nltk.word_tokenize(text))\n",
    "\n",
    "#Stemming\n",
    "df_Lied['processed_TEXT'] = df_Lied.processed_TEXT.apply(lambda text: stemm_List_of_Words(text))\n",
    "\n",
    "#remove stopwords\n",
    "df_Lied['processed_TEXT'] = df_Lied.processed_TEXT.apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "\n",
    "#remove numbers\n",
    "df_Lied['processed_TEXT'] = df_Lied.processed_TEXT.apply(lambda word_list : [re.sub('\\w*\\d\\w*','', word) for word in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LASS', 'STRAND', 'WEISST', 'WELCH', 'BRAUCH', 'MAL', 'WIED', 'MEER', 'SAND', 'YEAH', 'BRAUCH', 'MAL', 'WIEDERGRUND', 'BLEIB', 'AI', 'UHR', 'FAELLT', 'BLAU', 'SEH', 'VERSINKT', 'SETZT', 'LANGSAM', 'WASS', 'SPIEGELGLATT', 'BLINKT', 'WOLL', 'SEHNSUCHT', 'MORG', 'HEUT', 'PERFEKT', 'IS', 'SEHNSUCHT', 'MORG', 'HEUT', 'PERFEKT', 'IS', 'STEH', 'NEB', 'ZEIG', 'ECHT', 'LICHT', 'STADT', 'VERBRANNT', 'LASS', 'STRAND', 'KOMM', 'STEIG', 'LEUCHT', 'DIAMANT', 'YEH', 'ALL', 'SALZ', 'HAUT', 'AI', 'SEH', 'GESTOCH', 'SCHARF', 'TUT', 'FAST', 'AUG', 'WEH', 'ENDLICH', 'NOCH', 'NIE', 'GESEH', 'YEHI', 'WOLL', 'SEHNSUCHT', 'MORG', 'HEUT', 'PERFEKT', 'IS', 'SEHNSUCHT', 'MORG', 'HEUT', 'PERFEKT', 'IS', 'STEH', 'NEB', 'ZEIG', 'ECHT', 'LICHT', 'STADT', 'VERBRANNT', 'LASS', 'STRAND', 'LASS', 'STRAND', 'WEISST', 'WELCH', 'LASS', 'STRAND', 'YEAH', 'BRAUCHGRUND', 'BLEIB', 'AI', 'LASS', 'STRAND', 'WEISST', 'WELCH', 'BRAUCH', 'MAL', 'WIED', 'MEER', 'SAND', 'YEAH', 'BRAUCH', 'MAL', 'WIEDERGRUND', 'BLEIB', 'OUH', 'LASS', 'STRAND']\n"
     ]
    }
   ],
   "source": [
    "print(df_Lied['processed_TEXT'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN ICH BRAUCH MAL WIEDER MEER UND SAND  YEAH BRAUCH MAL WIEDERGRUND ZU BLEIBEN  AI DEINE UHR FAELLT INS BLAU WIR SEHEN  WIE DIE ZEIT VERSINKT WIR SETZTEN LANGSAM AUF DAS WASSER SPIEGELGLATT UND BLINKT  WIR WOLLEN DIE SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS DU STEHST NEBEN MIR  ZEIGST MIR  DASS ES ECHT IST DIE LICHTER MEINER STADT SIND VERBRANNT LASS UNS ZUM STRAND    KOMM WIR STEIGEN AUS DU LEUCHTEST WIE AUS DIAMANT  YEHE MIT ALL DEM SALZ AUF DER HAUT  AI ICH SEH DICH GESTOCHEN SCHARF ES TUT FAST DEN AUGEN WEH UND ENDLICH BIST DU DA SO HABE ICH DICH NOCH NIE GESEHEN  YEHI  WIR WOLLEN DIE SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS DU STEHST NEBEN MIR  ZEIGST MIR  DASS ES ECHT IST DIE LICHTER MEINER STADT SIND VERBRANNT LASS UNS ZUM STRAND   LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN LASS UNS ZUM STRAND  YEAH ICH BRAUCHGRUND ZU BLEIBEN  AI LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN ICH BRAUCH MAL WIEDER MEER UND SAND  YEAH BRAUCH MAL WIEDERGRUND ZU BLEIBEN  OUH LASS UNS ZUM STRAND \n"
     ]
    }
   ],
   "source": [
    "print(df_Lied['TEXT'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID       INTERPRET   TITEL  \\\n",
      "7219  408     Rumpelstilz  Kiosk    \n",
      "7220  408     Rumpelstilz  Kiosk    \n",
      "7763  425  Costa Cordalis  Anita    \n",
      "7762  425  Costa Cordalis  Anita    \n",
      "7761  425  Costa Cordalis  Anita    \n",
      "\n",
      "                                                   TEXT  \\\n",
      "7219  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "7220  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "7763  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "7762  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "7761  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "\n",
      "                                         processed_TEXT  POSITION  DATUM_VON  \\\n",
      "7219  [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...        50 2009-03-06   \n",
      "7220  [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...         4 1984-10-22   \n",
      "7763  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        23 2003-08-25   \n",
      "7762  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        39 2003-07-28   \n",
      "7761  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        27 2003-07-28   \n",
      "\n",
      "      DATUM_BIS  DAUER  JAHR  MONAT  \n",
      "7219 2009-03-12      6  2009      3  \n",
      "7220 1984-10-28      6  1984     10  \n",
      "7763 2003-08-31      6  2003      8  \n",
      "7762 2003-08-03      6  2003      8  \n",
      "7761 2003-08-03      6  2003      8  \n"
     ]
    }
   ],
   "source": [
    "df_Lied.drop(['TEXT_TEIL1','TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4', 'SPRACHE_DEUTSCH'], axis=1, inplace=True)\n",
    "df_Date = df_Chart_Position[['LIED_ID', 'DATUM_VON', 'DATUM_BIS', 'DAUER','JAHR', 'MONAT']]\n",
    "\n",
    "#not used anymore:\n",
    "#df_Chart_Position = df_Chart_Position.groupby('LIED_ID').agg({'POSITION':'mean','DAUER':'sum'}).reset_index()\n",
    "#df_Chart_Position['POSITION'] = df_Chart_Position.POSITION.apply(lambda pos: round(pos))\n",
    "#----\n",
    "\n",
    "df_Lied.sort_values(by='ID', inplace=True)\n",
    "df_Chart_Position.sort_values(by='LIED_ID', inplace=True)\n",
    "df = pd.concat([df_Lied, df_Chart_Position], axis='columns')\n",
    "df.drop('LIED_ID', axis=1, inplace=True)\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature Creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "df['ANZ_UNIQUE_WOERTER'] = list(len(set(word)) for word in df['processed_TEXT'])\n",
    "df['MAX_WORT_WDH'] = [max(Counter(text).values()) for text in df['processed_TEXT']]\n",
    "\n",
    "max_word_list = []\n",
    "for text in df['processed_TEXT']:\n",
    "    count = dict(Counter(text).items())\n",
    "    count = {k: v for k, v in sorted(count.items(), key=lambda item: item[1], reverse=True)}\n",
    "    key_list = [key for key in count.keys()]\n",
    "    values_list = [key for key in count.values()]\n",
    "    text_dict = {'WORD': key_list, 'FREQ': values_list}\n",
    "    max_word = key_list[0]\n",
    "    max_word_list.append(max_word)\n",
    "df['WORT_MAX_WDH'] = max_word_list\n",
    "\n",
    "df['LAENGE_LAENGSTES_WORT'] = list(len(max(set(word), key=len)) for word in df['processed_TEXT'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "            ID                    INTERPRET           TITEL  \\\n7219       408                  Rumpelstilz          Kiosk    \n7220       408                  Rumpelstilz          Kiosk    \n7763       425               Costa Cordalis          Anita    \n7762       425               Costa Cordalis          Anita    \n7761       425               Costa Cordalis          Anita    \n...        ...                          ...             ...   \n22129  2303823            AnnenMayKantereit  3 Tage am Meer   \n22014  2304169          Farid Bang & B-Case          Baller   \n22651  2304914                    Yung Hurn         Alleine   \n21906  2304939                  Azet & Zuna          Ohh oh   \n22150  2305285  Capital Bra & Lucry & Suena           Musik   \n\n                                                    TEXT  \\\n7219   ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n7220   ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n7763   JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n7762   JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n7761   JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n...                                                  ...   \n22129  ICH WEISS  WAS ICH KANN TROTZDEM AB UND AN KOM...   \n22014  BALLER   LA LA LA  ICH ZIEH  DIE WAFFE UND ICH...   \n22651  BABY  SAG MIR  WAS DU HEUTE MACHST HAST DU ZEI...   \n21906  OH  OH  AH OH   ULTRA PLUS  BABY     BIN MEIN ...   \n22150  BITTE BITTE GIB  MIR DAS GEFUEHL  DASS DU MICH...   \n\n                                          processed_TEXT  POSITION  DATUM_VON  \\\n7219   [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...        50 2009-03-06   \n7220   [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...         4 1984-10-22   \n7763   [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        23 2003-08-25   \n7762   [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        39 2003-07-28   \n7761   [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        27 2003-07-28   \n...                                                  ...       ...        ...   \n22129  [WEISS, KANN, TROTZD, KOMM, EN, ZWEIFEL, DARAN...        21 1980-11-24   \n22014  [BALL, LA, LA, LA, ZIEH, WAFF, BALL, LA, LA, L...        42 2020-04-10   \n22651  [BABY, SAG, HEUT, MACH, JA, BABY, SAG, HEUT, M...        20 2019-10-18   \n21906  [OH, OH, AH, OH, ULTRA, BABY, EIG, CHEF, SITZ,...         5 2019-07-26   \n22150  [BITT, BITT, GIB, GEFUEHL, NOCH, LIEB, TAG, WA...        38 2020-04-24   \n\n       DATUM_BIS  DAUER  JAHR  MONAT  ANZ_UNIQUE_WOERTER  MAX_WORT_WDH  \\\n7219  2009-03-12      6  2009      3                  83             6   \n7220  1984-10-28      6  1984     10                  83             6   \n7763  2003-08-31      6  2003      8                  73            16   \n7762  2003-08-03      6  2003      8                  73            16   \n7761  2003-08-03      6  2003      8                  73            16   \n...          ...    ...   ...    ...                 ...           ...   \n22129 1980-11-30      6  1980     11                  45            11   \n22014 2020-04-16      6  2020      4                 149            42   \n22651 2019-10-24      6  2019     10                  58            28   \n21906 2019-08-01      6  2019      8                 109            67   \n22150 2020-04-30      6  2020      4                 117            31   \n\n      WORT_MAX_WDH  LAENGE_LAENGSTES_WORT  RANK_SCORE  \n7219         KIOSK                     13           1  \n7220         KIOSK                     13          47  \n7763         ANITA                     12          28  \n7762         ANITA                     12          12  \n7761         ANITA                     12          24  \n...            ...                    ...         ...  \n22129         DREI                     11          30  \n22014           LA                     14           9  \n22651           JA                      7          31  \n21906           OH                     13          46  \n22150         BITT                     16          13  \n\n[22762 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>INTERPRET</th>\n      <th>TITEL</th>\n      <th>TEXT</th>\n      <th>processed_TEXT</th>\n      <th>POSITION</th>\n      <th>DATUM_VON</th>\n      <th>DATUM_BIS</th>\n      <th>DAUER</th>\n      <th>JAHR</th>\n      <th>MONAT</th>\n      <th>ANZ_UNIQUE_WOERTER</th>\n      <th>MAX_WORT_WDH</th>\n      <th>WORT_MAX_WDH</th>\n      <th>LAENGE_LAENGSTES_WORT</th>\n      <th>RANK_SCORE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7219</th>\n      <td>408</td>\n      <td>Rumpelstilz</td>\n      <td>Kiosk</td>\n      <td>ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...</td>\n      <td>[ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...</td>\n      <td>50</td>\n      <td>2009-03-06</td>\n      <td>2009-03-12</td>\n      <td>6</td>\n      <td>2009</td>\n      <td>3</td>\n      <td>83</td>\n      <td>6</td>\n      <td>KIOSK</td>\n      <td>13</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7220</th>\n      <td>408</td>\n      <td>Rumpelstilz</td>\n      <td>Kiosk</td>\n      <td>ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...</td>\n      <td>[ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...</td>\n      <td>4</td>\n      <td>1984-10-22</td>\n      <td>1984-10-28</td>\n      <td>6</td>\n      <td>1984</td>\n      <td>10</td>\n      <td>83</td>\n      <td>6</td>\n      <td>KIOSK</td>\n      <td>13</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>7763</th>\n      <td>425</td>\n      <td>Costa Cordalis</td>\n      <td>Anita</td>\n      <td>JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...</td>\n      <td>[JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...</td>\n      <td>23</td>\n      <td>2003-08-25</td>\n      <td>2003-08-31</td>\n      <td>6</td>\n      <td>2003</td>\n      <td>8</td>\n      <td>73</td>\n      <td>16</td>\n      <td>ANITA</td>\n      <td>12</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>7762</th>\n      <td>425</td>\n      <td>Costa Cordalis</td>\n      <td>Anita</td>\n      <td>JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...</td>\n      <td>[JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...</td>\n      <td>39</td>\n      <td>2003-07-28</td>\n      <td>2003-08-03</td>\n      <td>6</td>\n      <td>2003</td>\n      <td>8</td>\n      <td>73</td>\n      <td>16</td>\n      <td>ANITA</td>\n      <td>12</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>7761</th>\n      <td>425</td>\n      <td>Costa Cordalis</td>\n      <td>Anita</td>\n      <td>JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...</td>\n      <td>[JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...</td>\n      <td>27</td>\n      <td>2003-07-28</td>\n      <td>2003-08-03</td>\n      <td>6</td>\n      <td>2003</td>\n      <td>8</td>\n      <td>73</td>\n      <td>16</td>\n      <td>ANITA</td>\n      <td>12</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22129</th>\n      <td>2303823</td>\n      <td>AnnenMayKantereit</td>\n      <td>3 Tage am Meer</td>\n      <td>ICH WEISS  WAS ICH KANN TROTZDEM AB UND AN KOM...</td>\n      <td>[WEISS, KANN, TROTZD, KOMM, EN, ZWEIFEL, DARAN...</td>\n      <td>21</td>\n      <td>1980-11-24</td>\n      <td>1980-11-30</td>\n      <td>6</td>\n      <td>1980</td>\n      <td>11</td>\n      <td>45</td>\n      <td>11</td>\n      <td>DREI</td>\n      <td>11</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>22014</th>\n      <td>2304169</td>\n      <td>Farid Bang &amp; B-Case</td>\n      <td>Baller</td>\n      <td>BALLER   LA LA LA  ICH ZIEH  DIE WAFFE UND ICH...</td>\n      <td>[BALL, LA, LA, LA, ZIEH, WAFF, BALL, LA, LA, L...</td>\n      <td>42</td>\n      <td>2020-04-10</td>\n      <td>2020-04-16</td>\n      <td>6</td>\n      <td>2020</td>\n      <td>4</td>\n      <td>149</td>\n      <td>42</td>\n      <td>LA</td>\n      <td>14</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>22651</th>\n      <td>2304914</td>\n      <td>Yung Hurn</td>\n      <td>Alleine</td>\n      <td>BABY  SAG MIR  WAS DU HEUTE MACHST HAST DU ZEI...</td>\n      <td>[BABY, SAG, HEUT, MACH, JA, BABY, SAG, HEUT, M...</td>\n      <td>20</td>\n      <td>2019-10-18</td>\n      <td>2019-10-24</td>\n      <td>6</td>\n      <td>2019</td>\n      <td>10</td>\n      <td>58</td>\n      <td>28</td>\n      <td>JA</td>\n      <td>7</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>21906</th>\n      <td>2304939</td>\n      <td>Azet &amp; Zuna</td>\n      <td>Ohh oh</td>\n      <td>OH  OH  AH OH   ULTRA PLUS  BABY     BIN MEIN ...</td>\n      <td>[OH, OH, AH, OH, ULTRA, BABY, EIG, CHEF, SITZ,...</td>\n      <td>5</td>\n      <td>2019-07-26</td>\n      <td>2019-08-01</td>\n      <td>6</td>\n      <td>2019</td>\n      <td>8</td>\n      <td>109</td>\n      <td>67</td>\n      <td>OH</td>\n      <td>13</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>22150</th>\n      <td>2305285</td>\n      <td>Capital Bra &amp; Lucry &amp; Suena</td>\n      <td>Musik</td>\n      <td>BITTE BITTE GIB  MIR DAS GEFUEHL  DASS DU MICH...</td>\n      <td>[BITT, BITT, GIB, GEFUEHL, NOCH, LIEB, TAG, WA...</td>\n      <td>38</td>\n      <td>2020-04-24</td>\n      <td>2020-04-30</td>\n      <td>6</td>\n      <td>2020</td>\n      <td>4</td>\n      <td>117</td>\n      <td>31</td>\n      <td>BITT</td>\n      <td>16</td>\n      <td>13</td>\n    </tr>\n  </tbody>\n</table>\n<p>22762 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RANK_SCORES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "MAX_RANK = 50\n",
    "df['RANK_SCORE'] = MAX_RANK - df['POSITION'] + 1\n",
    "df['MAX_RANK_SCORE'] = [max(df.loc[df.ID == id_, 'RANK_SCORE']) for id_ in df['ID']]\n",
    "df['MEAN_RANK_SCORE'] = [round(mean(df.loc[df.ID == id_, 'RANK_SCORE'])) for id_ in df['ID']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Percentage of Stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "df['NUMBER_OF_STOPWORDS'] = df.TEXT.str.split().apply(lambda x: len(set(x) & set(stopwords_list)))\n",
    "df['STOPWORD_PERCENTAGE'] = df.NUMBER_OF_STOPWORDS.apply(lambda row: round(row/len(df['TEXT']), ndigits=5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "df.drop('NUMBER_OF_STOPWORDS', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID       INTERPRET   TITEL  \\\n",
      "0  408     Rumpelstilz  Kiosk    \n",
      "1  408     Rumpelstilz  Kiosk    \n",
      "2  425  Costa Cordalis  Anita    \n",
      "3  425  Costa Cordalis  Anita    \n",
      "4  425  Costa Cordalis  Anita    \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "1  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "2  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "3  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "4  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "\n",
      "                                      processed_TEXT  POSITION  DATUM_VON  \\\n",
      "0  [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...        50 2009-03-06   \n",
      "1  [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...         4 1984-10-22   \n",
      "2  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        23 2003-08-25   \n",
      "3  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        39 2003-07-28   \n",
      "4  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        27 2003-07-28   \n",
      "\n",
      "   DATUM_BIS  DAUER  JAHR  MONAT  ANZ_UNIQUE_WOERTER  MAX_WORT_WDH  \\\n",
      "0 2009-03-12      6  2009      3                  83             6   \n",
      "1 1984-10-28      6  1984     10                  83             6   \n",
      "2 2003-08-31      6  2003      8                  73            16   \n",
      "3 2003-08-03      6  2003      8                  73            16   \n",
      "4 2003-08-03      6  2003      8                  73            16   \n",
      "\n",
      "  WORT_MAX_WDH  LAENGE_LAENGSTES_WORT  RANK_SCORE  STOPWORD_PERCENTAGE  \n",
      "0        KIOSK                     13           1              0.00211  \n",
      "1        KIOSK                     13          47              0.00211  \n",
      "2        ANITA                     12          28              0.00189  \n",
      "3        ANITA                     12          12              0.00189  \n",
      "4        ANITA                     12          24              0.00189  \n"
     ]
    }
   ],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.drop('index',axis=1, inplace=True)\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize title\n",
    "#df['processed_TITLE'] = df.TITEL.apply(lambda titel: ' '.join([x.lemma_ for x in nlp(titel)]))\n",
    "#print(df['processed_TITLE'].head())\n",
    "\n",
    "#tokenize title\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['processed_TITLE'] = df.TITEL.apply(lambda titel: [word for word in tokenizer.tokenize(titel)])\n",
    "\n",
    "#Stemming\n",
    "df['processed_TITLE'] = df.processed_TITLE.apply(lambda titel: stemm_List_of_Words(titel))\n",
    "\n",
    "#remove stopwords\n",
    "df['processed_TITLE'] = df.processed_TITLE.apply(lambda titel: [word for word in titel if word.upper() not in stopwords_list])\n",
    "\n",
    "#remove numbers\n",
    "df['processed_TITLE'] = df.processed_TITLE.apply(lambda word_list : [re.sub('\\w*\\d\\w*','NUMBER', word) for word in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "df['LENGTH_TITLE'] = df.TITEL.apply(lambda titel: len(titel))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID       INTERPRET   TITEL  \\\n",
      "0  408     Rumpelstilz  Kiosk    \n",
      "1  408     Rumpelstilz  Kiosk    \n",
      "2  425  Costa Cordalis  Anita    \n",
      "3  425  Costa Cordalis  Anita    \n",
      "4  425  Costa Cordalis  Anita    \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "1  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "2  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "3  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "4  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "\n",
      "                                      processed_TEXT  POSITION  DATUM_VON  \\\n",
      "0  [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...        50 2009-03-06   \n",
      "1  [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...         4 1984-10-22   \n",
      "2  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        23 2003-08-25   \n",
      "3  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        39 2003-07-28   \n",
      "4  [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...        27 2003-07-28   \n",
      "\n",
      "   DATUM_BIS  DAUER  JAHR  MONAT  ANZ_UNIQUE_WOERTER  MAX_WORT_WDH  \\\n",
      "0 2009-03-12      6  2009      3                  83             6   \n",
      "1 1984-10-28      6  1984     10                  83             6   \n",
      "2 2003-08-31      6  2003      8                  73            16   \n",
      "3 2003-08-03      6  2003      8                  73            16   \n",
      "4 2003-08-03      6  2003      8                  73            16   \n",
      "\n",
      "  WORT_MAX_WDH  LAENGE_LAENGSTES_WORT  RANK_SCORE  STOPWORD_PERCENTAGE  \\\n",
      "0        KIOSK                     13           1              0.00211   \n",
      "1        KIOSK                     13          47              0.00211   \n",
      "2        ANITA                     12          28              0.00189   \n",
      "3        ANITA                     12          12              0.00189   \n",
      "4        ANITA                     12          24              0.00189   \n",
      "\n",
      "  processed_TITLE  LENGTH_TITLE  \n",
      "0         [KIOSK]             6  \n",
      "1         [KIOSK]             6  \n",
      "2         [ANITA]             6  \n",
      "3         [ANITA]             6  \n",
      "4         [ANITA]             6  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     WORD  FREQ\n",
      "0  NUMBER  1438\n",
      "1    LIEB   988\n",
      "2     ALL   452\n",
      "3   NACHT   417\n",
      "4     NUR   375\n"
     ]
    }
   ],
   "source": [
    "title_list = [title for title in df['processed_TITLE']]\n",
    "count = dict(Counter(flatten(title_list)).items())\n",
    "count = {k: v for k, v in sorted(count.items(), key=lambda item: item[1], reverse=True)}\n",
    "key_list = [key for key in count.keys()]\n",
    "values_list = [key for key in count.values()]\n",
    "title_dict = {'WORD': key_list, 'FREQ': values_list}\n",
    "df_Titel = pd.DataFrame(title_dict)\n",
    "print(df_Titel.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonal determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0        False\n1        False\n2        False\n3        False\n4        False\n         ...  \n22757    False\n22758    False\n22759    False\n22760    False\n22761    False\nName: SEASONAL?, Length: 22762, dtype: object"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SEASONAL?'] = np.nan\n",
    "isSeasonal()\n",
    "df['SEASONAL?']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "MULTILINGUAL DETECTION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "df['MULTILINGAL?'] = np.nan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22762, 23)\n"
     ]
    }
   ],
   "source": [
    "new_cols = ['ID', 'INTERPRET', 'TITEL', 'processed_TITLE', 'TEXT', 'processed_TEXT', 'DATUM_VON', 'DATUM_BIS', 'JAHR', 'MONAT', 'DAUER','ANZ_UNIQUE_WOERTER', 'MAX_WORT_WDH', 'WORT_MAX_WDH','LAENGE_LAENGSTES_WORT', 'STOPWORD_PERCENTAGE', 'LENGTH_TITLE', 'SEASONAL?', 'MULTILINGAL?', 'POSITION', 'RANK_SCORE', 'MAX_RANK_SCORE', 'MEAN_RANK_SCORE']\n",
    "df=df[new_cols]\n",
    "df=df.reindex(columns=new_cols)\n",
    "print(np.shape(df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "            ID                    INTERPRET           TITEL  \\\n0          408                  Rumpelstilz          Kiosk    \n1          408                  Rumpelstilz          Kiosk    \n2          425               Costa Cordalis          Anita    \n3          425               Costa Cordalis          Anita    \n4          425               Costa Cordalis          Anita    \n...        ...                          ...             ...   \n22757  2303823            AnnenMayKantereit  3 Tage am Meer   \n22758  2304169          Farid Bang & B-Case          Baller   \n22759  2304914                    Yung Hurn         Alleine   \n22760  2304939                  Azet & Zuna          Ohh oh   \n22761  2305285  Capital Bra & Lucry & Suena           Musik   \n\n           processed_TITLE                                               TEXT  \\\n0                  [KIOSK]  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n1                  [KIOSK]  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n2                  [ANITA]  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n3                  [ANITA]  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n4                  [ANITA]  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n...                    ...                                                ...   \n22757  [NUMBER, TAG, MEER]  ICH WEISS  WAS ICH KANN TROTZDEM AB UND AN KOM...   \n22758               [BALL]  BALLER   LA LA LA  ICH ZIEH  DIE WAFFE UND ICH...   \n22759             [ALLEIN]  BABY  SAG MIR  WAS DU HEUTE MACHST HAST DU ZEI...   \n22760            [OHH, OH]  OH  OH  AH OH   ULTRA PLUS  BABY     BIN MEIN ...   \n22761              [MUSIK]  BITTE BITTE GIB  MIR DAS GEFUEHL  DASS DU MICH...   \n\n                                          processed_TEXT  DATUM_VON  \\\n0      [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S... 2009-03-06   \n1      [ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S... 1984-10-22   \n2      [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,... 2003-08-25   \n3      [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,... 2003-07-28   \n4      [JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,... 2003-07-28   \n...                                                  ...        ...   \n22757  [WEISS, KANN, TROTZD, KOMM, EN, ZWEIFEL, DARAN... 1980-11-24   \n22758  [BALL, LA, LA, LA, ZIEH, WAFF, BALL, LA, LA, L... 2020-04-10   \n22759  [BABY, SAG, HEUT, MACH, JA, BABY, SAG, HEUT, M... 2019-10-18   \n22760  [OH, OH, AH, OH, ULTRA, BABY, EIG, CHEF, SITZ,... 2019-07-26   \n22761  [BITT, BITT, GIB, GEFUEHL, NOCH, LIEB, TAG, WA... 2020-04-24   \n\n       DATUM_BIS  JAHR  MONAT  ...  ANZ_UNIQUE_WOERTER  MAX_WORT_WDH  \\\n0     2009-03-12  2009      3  ...                  83             6   \n1     1984-10-28  1984     10  ...                  83             6   \n2     2003-08-31  2003      8  ...                  73            16   \n3     2003-08-03  2003      8  ...                  73            16   \n4     2003-08-03  2003      8  ...                  73            16   \n...          ...   ...    ...  ...                 ...           ...   \n22757 1980-11-30  1980     11  ...                  45            11   \n22758 2020-04-16  2020      4  ...                 149            42   \n22759 2019-10-24  2019     10  ...                  58            28   \n22760 2019-08-01  2019      8  ...                 109            67   \n22761 2020-04-30  2020      4  ...                 117            31   \n\n       WORT_MAX_WDH LAENGE_LAENGSTES_WORT  STOPWORD_PERCENTAGE  LENGTH_TITLE  \\\n0             KIOSK                    13              0.00211             6   \n1             KIOSK                    13              0.00211             6   \n2             ANITA                    12              0.00189             6   \n3             ANITA                    12              0.00189             6   \n4             ANITA                    12              0.00189             6   \n...             ...                   ...                  ...           ...   \n22757          DREI                    11              0.00114            14   \n22758            LA                    14              0.00264             6   \n22759            JA                     7              0.00185             7   \n22760            OH                    13              0.00224             6   \n22761          BITT                    16              0.00211             5   \n\n       SEASONAL? MULTILINGAL?  POSITION  RANK_SCORE  \n0          False          NaN        50           1  \n1          False          NaN         4          47  \n2          False          NaN        23          28  \n3          False          NaN        39          12  \n4          False          NaN        27          24  \n...          ...          ...       ...         ...  \n22757      False          NaN        21          30  \n22758      False          NaN        42           9  \n22759      False          NaN        20          31  \n22760      False          NaN         5          46  \n22761      False          NaN        38          13  \n\n[22762 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>INTERPRET</th>\n      <th>TITEL</th>\n      <th>processed_TITLE</th>\n      <th>TEXT</th>\n      <th>processed_TEXT</th>\n      <th>DATUM_VON</th>\n      <th>DATUM_BIS</th>\n      <th>JAHR</th>\n      <th>MONAT</th>\n      <th>...</th>\n      <th>ANZ_UNIQUE_WOERTER</th>\n      <th>MAX_WORT_WDH</th>\n      <th>WORT_MAX_WDH</th>\n      <th>LAENGE_LAENGSTES_WORT</th>\n      <th>STOPWORD_PERCENTAGE</th>\n      <th>LENGTH_TITLE</th>\n      <th>SEASONAL?</th>\n      <th>MULTILINGAL?</th>\n      <th>POSITION</th>\n      <th>RANK_SCORE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>408</td>\n      <td>Rumpelstilz</td>\n      <td>Kiosk</td>\n      <td>[KIOSK]</td>\n      <td>ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...</td>\n      <td>[ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...</td>\n      <td>2009-03-06</td>\n      <td>2009-03-12</td>\n      <td>2009</td>\n      <td>3</td>\n      <td>...</td>\n      <td>83</td>\n      <td>6</td>\n      <td>KIOSK</td>\n      <td>13</td>\n      <td>0.00211</td>\n      <td>6</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>408</td>\n      <td>Rumpelstilz</td>\n      <td>Kiosk</td>\n      <td>[KIOSK]</td>\n      <td>ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...</td>\n      <td>[ALSO, SAMML, FUER, GUT, ZWECK, SAGT, FRITZ, S...</td>\n      <td>1984-10-22</td>\n      <td>1984-10-28</td>\n      <td>1984</td>\n      <td>10</td>\n      <td>...</td>\n      <td>83</td>\n      <td>6</td>\n      <td>KIOSK</td>\n      <td>13</td>\n      <td>0.00211</td>\n      <td>6</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>425</td>\n      <td>Costa Cordalis</td>\n      <td>Anita</td>\n      <td>[ANITA]</td>\n      <td>JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...</td>\n      <td>[JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...</td>\n      <td>2003-08-25</td>\n      <td>2003-08-31</td>\n      <td>2003</td>\n      <td>8</td>\n      <td>...</td>\n      <td>73</td>\n      <td>16</td>\n      <td>ANITA</td>\n      <td>12</td>\n      <td>0.00189</td>\n      <td>6</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>23</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>425</td>\n      <td>Costa Cordalis</td>\n      <td>Anita</td>\n      <td>[ANITA]</td>\n      <td>JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...</td>\n      <td>[JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...</td>\n      <td>2003-07-28</td>\n      <td>2003-08-03</td>\n      <td>2003</td>\n      <td>8</td>\n      <td>...</td>\n      <td>73</td>\n      <td>16</td>\n      <td>ANITA</td>\n      <td>12</td>\n      <td>0.00189</td>\n      <td>6</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>39</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>425</td>\n      <td>Costa Cordalis</td>\n      <td>Anita</td>\n      <td>[ANITA]</td>\n      <td>JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...</td>\n      <td>[JVUIOUGIVTOH, FAND, IRGENDWO, ALLEIN, MEXIKO,...</td>\n      <td>2003-07-28</td>\n      <td>2003-08-03</td>\n      <td>2003</td>\n      <td>8</td>\n      <td>...</td>\n      <td>73</td>\n      <td>16</td>\n      <td>ANITA</td>\n      <td>12</td>\n      <td>0.00189</td>\n      <td>6</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>27</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22757</th>\n      <td>2303823</td>\n      <td>AnnenMayKantereit</td>\n      <td>3 Tage am Meer</td>\n      <td>[NUMBER, TAG, MEER]</td>\n      <td>ICH WEISS  WAS ICH KANN TROTZDEM AB UND AN KOM...</td>\n      <td>[WEISS, KANN, TROTZD, KOMM, EN, ZWEIFEL, DARAN...</td>\n      <td>1980-11-24</td>\n      <td>1980-11-30</td>\n      <td>1980</td>\n      <td>11</td>\n      <td>...</td>\n      <td>45</td>\n      <td>11</td>\n      <td>DREI</td>\n      <td>11</td>\n      <td>0.00114</td>\n      <td>14</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>21</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>22758</th>\n      <td>2304169</td>\n      <td>Farid Bang &amp; B-Case</td>\n      <td>Baller</td>\n      <td>[BALL]</td>\n      <td>BALLER   LA LA LA  ICH ZIEH  DIE WAFFE UND ICH...</td>\n      <td>[BALL, LA, LA, LA, ZIEH, WAFF, BALL, LA, LA, L...</td>\n      <td>2020-04-10</td>\n      <td>2020-04-16</td>\n      <td>2020</td>\n      <td>4</td>\n      <td>...</td>\n      <td>149</td>\n      <td>42</td>\n      <td>LA</td>\n      <td>14</td>\n      <td>0.00264</td>\n      <td>6</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>42</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>22759</th>\n      <td>2304914</td>\n      <td>Yung Hurn</td>\n      <td>Alleine</td>\n      <td>[ALLEIN]</td>\n      <td>BABY  SAG MIR  WAS DU HEUTE MACHST HAST DU ZEI...</td>\n      <td>[BABY, SAG, HEUT, MACH, JA, BABY, SAG, HEUT, M...</td>\n      <td>2019-10-18</td>\n      <td>2019-10-24</td>\n      <td>2019</td>\n      <td>10</td>\n      <td>...</td>\n      <td>58</td>\n      <td>28</td>\n      <td>JA</td>\n      <td>7</td>\n      <td>0.00185</td>\n      <td>7</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>20</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>22760</th>\n      <td>2304939</td>\n      <td>Azet &amp; Zuna</td>\n      <td>Ohh oh</td>\n      <td>[OHH, OH]</td>\n      <td>OH  OH  AH OH   ULTRA PLUS  BABY     BIN MEIN ...</td>\n      <td>[OH, OH, AH, OH, ULTRA, BABY, EIG, CHEF, SITZ,...</td>\n      <td>2019-07-26</td>\n      <td>2019-08-01</td>\n      <td>2019</td>\n      <td>8</td>\n      <td>...</td>\n      <td>109</td>\n      <td>67</td>\n      <td>OH</td>\n      <td>13</td>\n      <td>0.00224</td>\n      <td>6</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>22761</th>\n      <td>2305285</td>\n      <td>Capital Bra &amp; Lucry &amp; Suena</td>\n      <td>Musik</td>\n      <td>[MUSIK]</td>\n      <td>BITTE BITTE GIB  MIR DAS GEFUEHL  DASS DU MICH...</td>\n      <td>[BITT, BITT, GIB, GEFUEHL, NOCH, LIEB, TAG, WA...</td>\n      <td>2020-04-24</td>\n      <td>2020-04-30</td>\n      <td>2020</td>\n      <td>4</td>\n      <td>...</td>\n      <td>117</td>\n      <td>31</td>\n      <td>BITT</td>\n      <td>16</td>\n      <td>0.00211</td>\n      <td>5</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>38</td>\n      <td>13</td>\n    </tr>\n  </tbody>\n</table>\n<p>22762 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data/processed/EDA.csv')\n",
    "#df_Date.to_csv('Data/processed/DATE.csv')\n",
    "#df_Titel.to_csv('Data/processed/TITLE-ANALYSIS.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
