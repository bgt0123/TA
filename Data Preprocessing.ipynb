{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [5], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtextblob\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TextBlob\n\u001B[1;32m---> 11\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mde_core_news_sm\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m RAW_DATA_PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/raw/\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\Lib\\site-packages\\spacy\\__init__.py:54\u001B[0m, in \u001B[0;36mload\u001B[1;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[0;32m     31\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     37\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[0;32m     38\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \n\u001B[0;32m     41\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\Lib\\site-packages\\spacy\\util.py:439\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[0;32m    437\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m OLD_MODEL_SHORTCUTS:\n\u001B[0;32m    438\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE941\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname, full\u001B[38;5;241m=\u001B[39mOLD_MODEL_SHORTCUTS[name]))  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n\u001B[1;32m--> 439\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE050\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n",
      "\u001B[1;31mOSError\u001B[0m: [E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from profanity import profanity\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "RAW_DATA_PATH = 'data/raw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID INTERPRET    TITEL SPRACHE_DEUTSCH  \\\n",
      "0  1148163     Yasha  Strand               Ja   \n",
      "1  1148163     Yasha  Strand               Ja   \n",
      "2  1148163     Yasha  Strand               Ja   \n",
      "3  1148163     Yasha  Strand               Ja   \n",
      "4  1148163     Yasha  Strand               Ja   \n",
      "\n",
      "                                          TEXT_TEIL1 TEXT_TEIL2 TEXT_TEIL3  \\\n",
      "0  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "1  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "2  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "3  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "4  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "\n",
      "  TEXT_TEIL4  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n",
      "   LIED_ID  POSITION   DATUM_VON   DATUM_BIS\n",
      "0    26024        26  1983-04-18  1983-04-24\n",
      "1     1073        46  1984-06-04  1984-06-10\n",
      "2     1131        33  1984-06-18  1984-06-24\n",
      "3    15819        22  1984-08-13  1984-08-19\n",
      "4     7324         6  1984-08-27  1984-09-02\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "df_Lied = pd.read_csv(RAW_DATA_PATH + 'LIED.csv', usecols=['ID','INTERPRET', 'TITEL', 'SPRACHE_DEUTSCH', 'TEXT_TEIL1', 'TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4'])\n",
    "print(df_Lied.head())\n",
    "\n",
    "df_Chart_Position = pd.read_csv(RAW_DATA_PATH + 'CHART_POSITION.csv', usecols=['LIED_ID', 'POSITION', 'DATUM_VON', 'DATUM_BIS'])\n",
    "print(df_Chart_Position.head())\n",
    "\n",
    "with open(RAW_DATA_PATH+'Stoppwords.csv', newline='', encoding='UTF-8') as f:\n",
    "    stopwords_list = list(csv.reader(f))\n",
    "stopwords_list = [word.upper() for word in flatten(stopwords_list)]\n",
    "print(type(stopwords_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Lied['Text'] = df_Lied['TEXT_TEIL1'].fillna('') + df_Lied['TEXT_TEIL2'].fillna('') + df_Lied['TEXT_TEIL3'].fillna('') + df_Lied['TEXT_TEIL4'].fillna('')\n",
    "df_Lied.drop(columns=['TEXT_TEIL1', 'TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4'], axis=1, inplace=True)\n",
    "\n",
    "df_Chart_Position['DATUM_VON'] = pd.to_datetime(df_Chart_Position['DATUM_VON'])\n",
    "df_Chart_Position['DATUM_BIS'] = pd.to_datetime(df_Chart_Position['DATUM_BIS'])\n",
    "df_Chart_Position['DAUER'] = (df_Chart_Position['DATUM_BIS'] - df_Chart_Position['DATUM_VON']).dt.days.astype('int16')\n",
    "df_Chart_Position['Jahr'] = df_Chart_Position['DATUM_BIS'].dt.year.astype('int16')\n",
    "df_Chart_Position['Monat'] =  df_Chart_Position['DATUM_BIS'].dt.month.astype('int16')\n",
    "\n",
    "#df_Lied['TEXT_LAENGE'] = list(len(word) for word in df_Lied['Text'])\n",
    "df_Lied['ANZ_UNIQUE_WOERTER'] = list(len(set(word.split(\" \"))) for word in df_Lied['Text'])\n",
    "\n",
    "MAX_RANK = 50\n",
    "df_Chart_Position['RANK_SCORE'] = MAX_RANK - df_Chart_Position['POSITION'] + 1\n",
    "\n",
    "#df_copy_Chart_Position = df_Chart_Position[\"LIED_ID\"]\n",
    "\n",
    "#df_Chart_Position_Duplicates = df_Chart_Position[df_copy_Chart_Position.isin(df_copy_Chart_Position[df_copy_Chart_Position.duplicated()])].sort_values(\"LIED_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EY  SHAWTY WILL ALLEINE  SEIN  ALLEIN  DENN SIE HAT GERADE BAD VIBES SHAWTY WILL ALLEINE  SEIN  ALLEIN  IST NICHT ERREICHBAR AUF FACETIME  YEAH  MIR GENUEGT SCHON EIN CALL VON DIR  BABY  VON DIR  BABY  KEINER WEISS  WIESO  DOCH DU MACHST MICH SO CRAZY  KEINER WEISS  WIESO  EY  SHAWTY WILL ALLEINE  SEIN  ALLEIN  DENN SIE HAT GERADE BAD VIBES SIE HAT BAD VIBES  BAD VIBES   ES IST LATE NIGHT  NIGHT  ERFUELL IHR JEDEN WUNSCH  DENN WIR LEBEN DAS FAST LIFE  OH  JA  ICH WEISS  ALLE MACHEN AUGE AUF MEIN BABY  BABY  GIB MIR DEIN WORT UND ICH MACH DICH ZU MEINER LADY  JA  VERMISS DICH SCHON  JA  SOBALD ICH AUS DEM HAUS LAUF  OH OH OH  DU FICKST MEIN  N KOPF IRGENDWIE  ABER ICH STEH DRAUF  ICH STEH DRAUF  DU GEHST MIR AUS DEM WEG UND SUCHST DANN WIEDER NAEHE  OH YEAH  WAS DU DIR DABEI DENKST  DAS MUSS MAN NICHT VERSTEHEN MIT DIR FUEHL ICH MICH  ALS WAER ICH IM FINALE  FINALE  SCHEISS AUF ALLES  BRAUCH NUR DICH  KEINE POKALE  POKALE  DU REDEST HIN UND HER UND SENDEST MIR SIGNALE  OHH  DASS DU MICH ZAPPELN LAESST  DAS WISSEN WIR DOCH ALLE SHAWTY WILL ALLEINE  SEIN  ALLEIN  DENN SIE HAT GERADE BAD VIBES SHAWTY WILL ALLEINE  SEIN IST NICHT ERREICHBAR AUF FACETIME MIR GENUEGT SCHON EIN CALL VON DIR  BABY  VON DIR  BABY  KEINER WEISS  WIESO  DOCH DU MACHST MICH SO CRAZY  KEINER WEISS  WIESO  EY  SHAWTY WILL ALLEINE  SEIN  ALLEIN  DENN SIE HAT GERADE BAD VIBES SHAWTY WILL ALLEINE  SEIN  ALLEIN SEIN  WEIL ICH WEISS  DASS SIE SICH GRAD IN   SCHLAF WEINT  JA  JA  JA  OKAY  AUGEN WIE EIN BERNSTEIN  OH WOAH  TREFFEN UNS BEI NACHT  HOEREN  MURDER ON MY MIND  DU UND ICH  WIR CONNECTEN WIE DAS WLAN  WLAN  ZWISCHENDURCH HAT MAL JEDER SEINE FEHLER  JEDER SEINE FEHLER  DU GEHST MIR AUS DEM WEG UND SUCHST DANN WIEDER NAEHE WAS DU DIR DABEI DENKST  DAS MUSS MAN NICHT VERSTEHEN MIT DIR FUEHL ICH MICH  ALS WAER ICH IM FINALE  FINALE  SCHEISS AUF ALLES  BRAUCH NUR DICH  KEINE POKALE  KEINE POKALE  DU REDEST HIN UND HER UND SENDEST MIR SIGNALE  SIGNALE  DASS DU MICH ZAPPELN LAESST  DAS WISSEN WIR DOCH ALLE SHAWTY WILL ALLEINE  SEIN  SHAWTY WILL ALLEINE  SEIN  DENN SIE HAT GERADE BAD VIBES  BAD VIBES  SHAWTY WILL ALLEINE  SEIN  ALLEIN SEIN  IST NICHT ERREICHBAR AUF FACETIME EY  SHAWTY WILL ALLEINE  SEIN  ALLEIN  DENN SIE HAT GERADE BAD VIBES SHAWTY WILL ALLEINE  SEIN IST NICHT ERREICHBAR AUF FACETIME MIR GENUEGT SCHON EIN CALL VON DIR  BABY  VON DIR  BABY  KEINER WEISS  WIESO  DOCH DU MACHST MICH SO CRAZY  KEINER WEISS  WIESO  EY  SHAWTY WILL ALLEINE  SEIN  ALLEIN  DENN SIE HAT GERADE BAD VIBES\n"
     ]
    }
   ],
   "source": [
    "print(df_Lied['Text'][1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'de_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [128], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#lemmatization\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mde_core_news_md\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m mails_lemma \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m df_Lied:\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\Lib\\site-packages\\spacy\\__init__.py:54\u001B[0m, in \u001B[0;36mload\u001B[1;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[0;32m     31\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     37\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[0;32m     38\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \n\u001B[0;32m     41\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\venv\\Lib\\site-packages\\spacy\\util.py:439\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[0;32m    437\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m OLD_MODEL_SHORTCUTS:\n\u001B[0;32m    438\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE941\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname, full\u001B[38;5;241m=\u001B[39mOLD_MODEL_SHORTCUTS[name]))  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n\u001B[1;32m--> 439\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE050\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n",
      "\u001B[1;31mOSError\u001B[0m: [E050] Can't find model 'de_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "#df_Lied['lemmatized_text'] = df_Lied.text.apply(lambda row: [token.lemma_ for token in nlprow['Text'])])\n",
    "\n",
    "# tokenize\n",
    "#df_Lied['tokenized_text'] = df_Lied.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
    "\n",
    "#remove stopwords\n",
    "#df_Lied['tokenized_text'] = df_Lied['tokenized_text'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "\n",
    "#Stemming\n",
    "\n",
    "\n",
    "# ngram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EY', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'GERADE', 'BAD', 'VIBES', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'ERREICHBAR', 'FACETIME', 'YEAH', 'GENUEGT', 'SCHON', 'CALL', 'BABY', 'BABY', 'WEISS', 'DOCH', 'MACHST', 'CRAZY', 'WEISS', 'EY', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'GERADE', 'BAD', 'VIBES', 'BAD', 'VIBES', 'BAD', 'VIBES', 'LATE', 'NIGHT', 'NIGHT', 'ERFUELL', 'WUNSCH', 'LEBEN', 'FAST', 'LIFE', 'OH', 'JA', 'WEISS', 'MACHEN', 'AUGE', 'BABY', 'BABY', 'GIB', 'MACH', 'LADY', 'JA', 'VERMISS', 'SCHON', 'JA', 'HAUS', 'LAUF', 'OH', 'OH', 'OH', 'FICKST', 'KOPF', 'IRGENDWIE', 'STEH', 'DRAUF', 'STEH', 'DRAUF', 'GEHST', 'WEG', 'SUCHST', 'WIEDER', 'NAEHE', 'OH', 'YEAH', 'DABEI', 'DENKST', 'MUSS', 'VERSTEHEN', 'FUEHL', 'WAER', 'FINALE', 'FINALE', 'SCHEISS', 'BRAUCH', 'NUR', 'POKALE', 'POKALE', 'REDEST', 'HIN', 'HER', 'SENDEST', 'SIGNALE', 'OHH', 'ZAPPELN', 'LAESST', 'WISSEN', 'DOCH', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'GERADE', 'BAD', 'VIBES', 'SHAWTY', 'WILL', 'ALLEINE', 'ERREICHBAR', 'FACETIME', 'GENUEGT', 'SCHON', 'CALL', 'BABY', 'BABY', 'WEISS', 'DOCH', 'MACHST', 'CRAZY', 'WEISS', 'EY', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'GERADE', 'BAD', 'VIBES', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'WEISS', 'GRAD', 'SCHLAF', 'WEINT', 'JA', 'JA', 'JA', 'OKAY', 'AUGEN', 'BERNSTEIN', 'OH', 'WOAH', 'TREFFEN', 'NACHT', 'HOEREN', 'MURDER', 'ON', 'MY', 'MIND', 'CONNECTEN', 'WLAN', 'WLAN', 'ZWISCHENDURCH', 'MAL', 'FEHLER', 'FEHLER', 'GEHST', 'WEG', 'SUCHST', 'WIEDER', 'NAEHE', 'DABEI', 'DENKST', 'MUSS', 'VERSTEHEN', 'FUEHL', 'WAER', 'FINALE', 'FINALE', 'SCHEISS', 'BRAUCH', 'NUR', 'POKALE', 'POKALE', 'REDEST', 'HIN', 'HER', 'SENDEST', 'SIGNALE', 'SIGNALE', 'ZAPPELN', 'LAESST', 'WISSEN', 'DOCH', 'SHAWTY', 'WILL', 'ALLEINE', 'SHAWTY', 'WILL', 'ALLEINE', 'GERADE', 'BAD', 'VIBES', 'BAD', 'VIBES', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'ERREICHBAR', 'FACETIME', 'EY', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'GERADE', 'BAD', 'VIBES', 'SHAWTY', 'WILL', 'ALLEINE', 'ERREICHBAR', 'FACETIME', 'GENUEGT', 'SCHON', 'CALL', 'BABY', 'BABY', 'WEISS', 'DOCH', 'MACHST', 'CRAZY', 'WEISS', 'EY', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'GERADE', 'BAD', 'VIBES']\n"
     ]
    }
   ],
   "source": [
    "print(df_Lied['tokenized_text'][1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "profanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(RAW_DATA_PATH+'german-profanity-list.csv', newline='', encoding='UTF-8') as f:\n",
    "    profanity_list = list(csv.reader(f))\n",
    "\n",
    "profanity.load_words(flatten(profanity_list))\n",
    "profanity.contains_profanity(df_Lied['Text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA (BoW) - Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "titel_list = [word.upper() for word in list(set(df_Lied['TITEL']))]\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_list = [word for titel in titel_list for word in tokenizer.tokenize(titel)]\n",
    "no_stopwords_title_list = [item for item in tokenized_list if item not in stopwords_list]\n",
    "df_title_word_occurrence = pd.value_counts(np.array(no_stopwords_title_list))\n",
    "df_title_word_occurrence.to_csv('Data/processed/title_word_occurrence.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonal determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted Words -Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.merge(df_Lied, df_Chart_Position.set_axis(['DATUM_VON','DATUM_BIS', 'ID', 'POSITION', 'DAUER', 'BELIEBTHEIT', 'Jahr', 'Monat'], axis=1, inplace=False), on='ID', how='inner').sort_values(by=['ID'])\n",
    "print(df_Lied.isnull().values.any())#df muss noch ausgetauscht werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
