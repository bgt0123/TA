{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "from profanity import profanity\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "RAW_DATA_PATH = 'data/raw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def find_longest_word(word_list):\n",
    "    longest_word =  max(word_list, key=len)\n",
    "    return len(longest_word)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID INTERPRET    TITEL SPRACHE_DEUTSCH  \\\n",
      "0  1148163     Yasha  Strand               Ja   \n",
      "1  1148163     Yasha  Strand               Ja   \n",
      "2  1148163     Yasha  Strand               Ja   \n",
      "3  1148163     Yasha  Strand               Ja   \n",
      "4  1148163     Yasha  Strand               Ja   \n",
      "\n",
      "                                          TEXT_TEIL1 TEXT_TEIL2 TEXT_TEIL3  \\\n",
      "0  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "1  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "2  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "3  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "4  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "\n",
      "  TEXT_TEIL4  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n",
      "   LIED_ID  POSITION   DATUM_VON   DATUM_BIS\n",
      "0    26024        26  1983-04-18  1983-04-24\n",
      "1     1073        46  1984-06-04  1984-06-10\n",
      "2     1131        33  1984-06-18  1984-06-24\n",
      "3    15819        22  1984-08-13  1984-08-19\n",
      "4     7324         6  1984-08-27  1984-09-02\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "df_Lied = pd.read_csv(RAW_DATA_PATH + 'LIED.csv', usecols=['ID','INTERPRET', 'TITEL', 'SPRACHE_DEUTSCH', 'TEXT_TEIL1', 'TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4'])\n",
    "print(df_Lied.head())\n",
    "\n",
    "df_Chart_Position = pd.read_csv(RAW_DATA_PATH + 'CHART_POSITION.csv', usecols=['LIED_ID', 'POSITION', 'DATUM_VON', 'DATUM_BIS'])\n",
    "print(df_Chart_Position.head())\n",
    "\n",
    "with open(RAW_DATA_PATH+'Stoppwords.csv', newline='', encoding='UTF-8') as f:\n",
    "    stopwords_list = list(csv.reader(f))\n",
    "stopwords_list = [word.upper() for word in flatten(stopwords_list)]\n",
    "print(type(stopwords_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Lied['Text'] = df_Lied['TEXT_TEIL1'].fillna('') + df_Lied['TEXT_TEIL2'].fillna('') + df_Lied['TEXT_TEIL3'].fillna('') + df_Lied['TEXT_TEIL4'].fillna('')\n",
    "#df_Lied.drop(columns=['TEXT_TEIL1', 'TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4'], axis=1, inplace=True)\n",
    "\n",
    "df_Chart_Position['DATUM_VON'] = pd.to_datetime(df_Chart_Position['DATUM_VON'])\n",
    "df_Chart_Position['DATUM_BIS'] = pd.to_datetime(df_Chart_Position['DATUM_BIS'])\n",
    "df_Chart_Position['DAUER'] = (df_Chart_Position['DATUM_BIS'] - df_Chart_Position['DATUM_VON']).dt.days.astype('int16')\n",
    "df_Chart_Position['Jahr'] = df_Chart_Position['DATUM_BIS'].dt.year.astype('int16')\n",
    "df_Chart_Position['Monat'] =  df_Chart_Position['DATUM_BIS'].dt.month.astype('int16')\n",
    "\n",
    "#df_Lied['TEXT_LAENGE'] = list(len(word) for word in df_Lied['Text'])\n",
    "df_Lied['ANZ_UNIQUE_WOERTER'] = list(len(set(word.split(\" \"))) for word in df_Lied['Text'])\n",
    "df_Lied['LAENGE_LAENGSTES_WORT'] = list(len(max(set(word.split(\" \")), key=len)) for word in df_Lied['Text'])\n",
    "df_Lied[['LAENGE_LAENGSTES_WORT','ID']].to_csv('Data/processed/Wort_LAENGE_v2.csv')\n",
    "df_Lied['INTERPRET'].value_counts(ascending=True, dropna=True, normalize=True)\\\n",
    "    #.to_csv('Data/processed/Interpret_Occurrences_normalised.csv')\n",
    "\n",
    "MAX_RANK = 50\n",
    "df_Chart_Position['RANK_SCORE'] = MAX_RANK - df_Chart_Position['POSITION'] + 1\n",
    "\n",
    "#df_copy_Chart_Position = df_Chart_Position[\"LIED_ID\"]\n",
    "\n",
    "#df_Chart_Position_Duplicates = df_Chart_Position[df_copy_Chart_Position.isin(df_copy_Chart_Position[df_copy_Chart_Position.duplicated()])].sort_values(\"LIED_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EY  SHAWTY WILL ALLEINE  SEIN  ALLEIN  DENN SIE HAT GERADE BAD VIBES SHAWTY WILL ALLEINE  SEIN  ALLEIN  IST NICHT ERREICHBAR AUF FACETIME  YEAH  MIR GENUEGT SCHON EIN CALL VON DIR  BABY  VON DIR  BABY  KEINER WEISS  WIESO  DOCH DU MACHST MICH SO CRAZY  KEINER WEISS  WIESO  EY  SHAWTY WILL ALLEINE  SEIN  ALLEIN  DENN SIE HAT GERADE BAD VIBES SIE HAT BAD VIBES  BAD VIBES   ES IST LATE NIGHT  NIGHT  ERFUELL IHR JEDEN WUNSCH  DENN WIR LEBEN DAS FAST LIFE  OH  JA  ICH WEISS  ALLE MACHEN AUGE AUF MEIN BABY  BABY  GIB MIR DEIN WORT UND ICH MACH DICH ZU MEINER LADY  JA  VERMISS DICH SCHON  JA  SOBALD ICH AUS DEM HAUS LAUF  OH OH OH  DU FICKST MEIN  N KOPF IRGENDWIE  ABER ICH STEH DRAUF  ICH STEH DRAUF  DU GEHST MIR AUS DEM WEG UND SUCHST DANN WIEDER NAEHE  OH YEAH  WAS DU DIR DABEI DENKST  DAS MUSS MAN NICHT VERSTEHEN MIT DIR FUEHL ICH MICH  ALS WAER ICH IM FINALE  FINALE  SCHEISS AUF ALLES  BRAUCH NUR DICH  KEINE POKALE  POKALE  DU REDEST HIN UND HER UND SENDEST MIR SIGNALE  OHH  DASS DU MICH ZAPPELN LAESST  DAS WISSEN WIR DOCH ALLE SHAWTY WILL ALLEINE  SEIN  ALLEIN  DENN SIE HAT GERADE BAD VIBES SHAWTY WILL ALLEINE  SEIN IST NICHT ERREICHBAR AUF FACETIME MIR GENUEGT SCHON EIN CALL VON DIR  BABY  VON DIR  BABY  KEINER WEISS  WIESO  DOCH DU MACHST MICH SO CRAZY  KEINER WEISS  WIESO  EY  SHAWTY WILL ALLEINE  SEIN  ALLEIN  DENN SIE HAT GERADE BAD VIBES SHAWTY WILL ALLEINE  SEIN  ALLEIN SEIN  WEIL ICH WEISS  DASS SIE SICH GRAD IN   SCHLAF WEINT  JA  JA  JA  OKAY  AUGEN WIE EIN BERNSTEIN  OH WOAH  TREFFEN UNS BEI NACHT  HOEREN  MURDER ON MY MIND  DU UND ICH  WIR CONNECTEN WIE DAS WLAN  WLAN  ZWISCHENDURCH HAT MAL JEDER SEINE FEHLER  JEDER SEINE FEHLER  DU GEHST MIR AUS DEM WEG UND SUCHST DANN WIEDER NAEHE WAS DU DIR DABEI DENKST  DAS MUSS MAN NICHT VERSTEHEN MIT DIR FUEHL ICH MICH  ALS WAER ICH IM FINALE  FINALE  SCHEISS AUF ALLES  BRAUCH NUR DICH  KEINE POKALE  KEINE POKALE  DU REDEST HIN UND HER UND SENDEST MIR SIGNALE  SIGNALE  DASS DU MICH ZAPPELN LAESST  DAS WISSEN WIR DOCH ALLE SHAWTY WILL ALLEINE  SEIN  SHAWTY WILL ALLEINE  SEIN  DENN SIE HAT GERADE BAD VIBES  BAD VIBES  SHAWTY WILL ALLEINE  SEIN  ALLEIN SEIN  IST NICHT ERREICHBAR AUF FACETIME EY  SHAWTY WILL ALLEINE  SEIN  ALLEIN  DENN SIE HAT GERADE BAD VIBES SHAWTY WILL ALLEINE  SEIN IST NICHT ERREICHBAR AUF FACETIME MIR GENUEGT SCHON EIN CALL VON DIR  BABY  VON DIR  BABY  KEINER WEISS  WIESO  DOCH DU MACHST MICH SO CRAZY  KEINER WEISS  WIESO  EY  SHAWTY WILL ALLEINE  SEIN  ALLEIN  DENN SIE HAT GERADE BAD VIBES\n"
     ]
    }
   ],
   "source": [
    "print(df_Lied['Text'][1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [12], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m df_Lied[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocessed_text\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df_Lied[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocessed_text\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: [item \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m x \u001B[38;5;28;01mif\u001B[39;00m item \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stopwords_list])\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m#remove numbers\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[43mdf_Lied\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mprocessed_text\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mw*\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43md\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mw*\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(df_Lied\u001B[38;5;241m.\u001B[39mhead())\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m#Stemming\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \n\u001B[0;32m     17\u001B[0m \n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# ngram\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4661\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4662\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4663\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4666\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4667\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4668\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4669\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4670\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4769\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4770\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1105\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[0;32m   1104\u001B[0m \u001B[38;5;66;03m# self.f is Callable\u001B[39;00m\n\u001B[1;32m-> 1105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1156\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1154\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1155\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[1;32m-> 1156\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1157\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1158\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1159\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1160\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1163\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1164\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1165\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn [12], line 12\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m      9\u001B[0m df_Lied[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocessed_text\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df_Lied[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocessed_text\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: [item \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m x \u001B[38;5;28;01mif\u001B[39;00m item \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stopwords_list])\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m#remove numbers\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m df_Lied[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocessed_text\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x : \u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mw*\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43md\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mw*\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(df_Lied\u001B[38;5;241m.\u001B[39mhead())\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m#Stemming\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \n\u001B[0;32m     17\u001B[0m \n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# ngram\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\re\\__init__.py:185\u001B[0m, in \u001B[0;36msub\u001B[1;34m(pattern, repl, string, count, flags)\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msub\u001B[39m(pattern, repl, string, count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, flags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    179\u001B[0m     \u001B[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001B[39;00m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001B[39;00m\n\u001B[0;32m    181\u001B[0m \u001B[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001B[39;00m\n\u001B[0;32m    182\u001B[0m \u001B[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001B[39;00m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;124;03m    a callable, it's passed the Match object and must return\u001B[39;00m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;124;03m    a replacement string to be used.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstring\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcount\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "\n",
    "#df_Lied['Text'].apply(lambda x: \" \".join([word.lemma_ for text in df_Lied['Text'] for word in nlp(text)]))\n",
    "\n",
    "# tokenize\n",
    "df_Lied['processed_text'] = df_Lied.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
    "\n",
    "#remove stopwords\n",
    "df_Lied['processed_text'] = df_Lied['processed_text'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "\n",
    "#remove numbers\n",
    "df_Lied['processed_text'].apply(lambda x : re.sub('\\w*\\d\\w*','', x))\n",
    "#print(df_Lied.head())\n",
    "\n",
    "#Stemming\n",
    "\n",
    "\n",
    "# ngram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EY', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'GERADE', 'BAD', 'VIBES', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'ERREICHBAR', 'FACETIME', 'YEAH', 'GENUEGT', 'SCHON', 'CALL', 'BABY', 'BABY', 'WEISS', 'DOCH', 'MACHST', 'CRAZY', 'WEISS', 'EY', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'GERADE', 'BAD', 'VIBES', 'BAD', 'VIBES', 'BAD', 'VIBES', 'LATE', 'NIGHT', 'NIGHT', 'ERFUELL', 'WUNSCH', 'LEBEN', 'FAST', 'LIFE', 'OH', 'JA', 'WEISS', 'MACHEN', 'AUGE', 'BABY', 'BABY', 'GIB', 'MACH', 'LADY', 'JA', 'VERMISS', 'SCHON', 'JA', 'HAUS', 'LAUF', 'OH', 'OH', 'OH', 'FICKST', 'KOPF', 'IRGENDWIE', 'STEH', 'DRAUF', 'STEH', 'DRAUF', 'GEHST', 'WEG', 'SUCHST', 'WIEDER', 'NAEHE', 'OH', 'YEAH', 'DABEI', 'DENKST', 'MUSS', 'VERSTEHEN', 'FUEHL', 'WAER', 'FINALE', 'FINALE', 'SCHEISS', 'BRAUCH', 'NUR', 'POKALE', 'POKALE', 'REDEST', 'HIN', 'HER', 'SENDEST', 'SIGNALE', 'OHH', 'ZAPPELN', 'LAESST', 'WISSEN', 'DOCH', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'GERADE', 'BAD', 'VIBES', 'SHAWTY', 'WILL', 'ALLEINE', 'ERREICHBAR', 'FACETIME', 'GENUEGT', 'SCHON', 'CALL', 'BABY', 'BABY', 'WEISS', 'DOCH', 'MACHST', 'CRAZY', 'WEISS', 'EY', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'GERADE', 'BAD', 'VIBES', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'WEISS', 'GRAD', 'SCHLAF', 'WEINT', 'JA', 'JA', 'JA', 'OKAY', 'AUGEN', 'BERNSTEIN', 'OH', 'WOAH', 'TREFFEN', 'NACHT', 'HOEREN', 'MURDER', 'ON', 'MY', 'MIND', 'CONNECTEN', 'WLAN', 'WLAN', 'ZWISCHENDURCH', 'MAL', 'FEHLER', 'FEHLER', 'GEHST', 'WEG', 'SUCHST', 'WIEDER', 'NAEHE', 'DABEI', 'DENKST', 'MUSS', 'VERSTEHEN', 'FUEHL', 'WAER', 'FINALE', 'FINALE', 'SCHEISS', 'BRAUCH', 'NUR', 'POKALE', 'POKALE', 'REDEST', 'HIN', 'HER', 'SENDEST', 'SIGNALE', 'SIGNALE', 'ZAPPELN', 'LAESST', 'WISSEN', 'DOCH', 'SHAWTY', 'WILL', 'ALLEINE', 'SHAWTY', 'WILL', 'ALLEINE', 'GERADE', 'BAD', 'VIBES', 'BAD', 'VIBES', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'ERREICHBAR', 'FACETIME', 'EY', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'GERADE', 'BAD', 'VIBES', 'SHAWTY', 'WILL', 'ALLEINE', 'ERREICHBAR', 'FACETIME', 'GENUEGT', 'SCHON', 'CALL', 'BABY', 'BABY', 'WEISS', 'DOCH', 'MACHST', 'CRAZY', 'WEISS', 'EY', 'SHAWTY', 'WILL', 'ALLEINE', 'ALLEIN', 'GERADE', 'BAD', 'VIBES']\n"
     ]
    }
   ],
   "source": [
    "print(df_Lied['tokenized_text'][1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "profanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(RAW_DATA_PATH+'german-profanity-list.csv', newline='', encoding='UTF-8') as f:\n",
    "    profanity_list = list(csv.reader(f))\n",
    "\n",
    "profanity.load_words(flatten(profanity_list))\n",
    "profanity.contains_profanity(df_Lied['Text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Percentage of Stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "df_Lied['Number_of_Stopwords'] = df_Lied['Text'].str.split().apply(lambda x: len(set(x) & set(stopwords_list)))\n",
    "df_Lied['Stopword_Percentage'] = df_Lied.Number_of_Stopwords.apply(lambda row: row/len(df_Lied['Text']))\n",
    "df_Lied[['ID','Number_of_Stopwords','Stopword_Percentage']].to_csv('Data/processed/Stoppwords.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "        ID INTERPRET    TITEL SPRACHE_DEUTSCH  \\\n0  1148163     Yasha  Strand               Ja   \n1  1148163     Yasha  Strand               Ja   \n2  1148163     Yasha  Strand               Ja   \n3  1148163     Yasha  Strand               Ja   \n4  1148163     Yasha  Strand               Ja   \n\n                                          TEXT_TEIL1 TEXT_TEIL2 TEXT_TEIL3  \\\n0  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n1  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n2  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n3  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n4  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n\n  TEXT_TEIL4                                               Text  \\\n0        NaN  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...   \n1        NaN  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...   \n2        NaN  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...   \n3        NaN  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...   \n4        NaN  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...   \n\n   ANZ_UNIQUE_WOERTER  LAENGE_LAENGSTES_WORT  \\\n0                  92                     12   \n1                  92                     12   \n2                  92                     12   \n3                  92                     12   \n4                  92                     12   \n\n                                      processed_text  Number_of_Stopwords  \\\n0  [LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...                   35   \n1  [LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...                   35   \n2  [LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...                   35   \n3  [LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...                   35   \n4  [LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...                   35   \n\n   Stopword_Percentage  SUBJEKTIVITY  POLARITY  \n0             0.001538           0.6       0.2  \n1             0.001538           0.6       0.2  \n2             0.001538           0.6       0.2  \n3             0.001538           0.6       0.2  \n4             0.001538           0.6       0.2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>INTERPRET</th>\n      <th>TITEL</th>\n      <th>SPRACHE_DEUTSCH</th>\n      <th>TEXT_TEIL1</th>\n      <th>TEXT_TEIL2</th>\n      <th>TEXT_TEIL3</th>\n      <th>TEXT_TEIL4</th>\n      <th>Text</th>\n      <th>ANZ_UNIQUE_WOERTER</th>\n      <th>LAENGE_LAENGSTES_WORT</th>\n      <th>processed_text</th>\n      <th>Number_of_Stopwords</th>\n      <th>Stopword_Percentage</th>\n      <th>SUBJEKTIVITY</th>\n      <th>POLARITY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1148163</td>\n      <td>Yasha</td>\n      <td>Strand</td>\n      <td>Ja</td>\n      <td>LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...</td>\n      <td>92</td>\n      <td>12</td>\n      <td>[LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...</td>\n      <td>35</td>\n      <td>0.001538</td>\n      <td>0.6</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1148163</td>\n      <td>Yasha</td>\n      <td>Strand</td>\n      <td>Ja</td>\n      <td>LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...</td>\n      <td>92</td>\n      <td>12</td>\n      <td>[LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...</td>\n      <td>35</td>\n      <td>0.001538</td>\n      <td>0.6</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1148163</td>\n      <td>Yasha</td>\n      <td>Strand</td>\n      <td>Ja</td>\n      <td>LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...</td>\n      <td>92</td>\n      <td>12</td>\n      <td>[LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...</td>\n      <td>35</td>\n      <td>0.001538</td>\n      <td>0.6</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1148163</td>\n      <td>Yasha</td>\n      <td>Strand</td>\n      <td>Ja</td>\n      <td>LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...</td>\n      <td>92</td>\n      <td>12</td>\n      <td>[LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...</td>\n      <td>35</td>\n      <td>0.001538</td>\n      <td>0.6</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1148163</td>\n      <td>Yasha</td>\n      <td>Strand</td>\n      <td>Ja</td>\n      <td>LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...</td>\n      <td>92</td>\n      <td>12</td>\n      <td>[LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...</td>\n      <td>35</td>\n      <td>0.001538</td>\n      <td>0.6</td>\n      <td>0.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_Chart_Position.head()\n",
    "df_Lied.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_Lied['SUBJEKTIVITY'] = df_Lied.processed_text.apply(lambda text: TextBlob(str(text)).sentiment.subjectivity)\n",
    "df_Lied['POLARITY'] = df_Lied.processed_text.apply(lambda text: TextBlob(str(text)).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA (BoW) - Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.2, subjectivity=0.6)\n"
     ]
    }
   ],
   "source": [
    "print(TextBlob(str(df_Lied['processed_text'][1])).sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [32], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m test \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[43mword\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdf_Lied\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mText\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mword\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ments\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(test[\u001B[38;5;241m0\u001B[39m])\n",
      "Cell \u001B[1;32mIn [32], line 1\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m test \u001B[38;5;241m=\u001B[39m [word \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m df_Lied[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mText\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39ments]\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(test[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\spacy\\language.py:1026\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[1;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[0;32m   1024\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[0;32m   1025\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1026\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1028\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[0;32m   1029\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001B[0m, in \u001B[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\spacy\\pipeline\\edit_tree_lemmatizer.py:154\u001B[0m, in \u001B[0;36mEditTreeLemmatizer.predict\u001B[1;34m(self, docs)\u001B[0m\n\u001B[0;32m    152\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(guesses) \u001B[38;5;241m==\u001B[39m n_docs\n\u001B[0;32m    153\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m guesses\n\u001B[1;32m--> 154\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(scores) \u001B[38;5;241m==\u001B[39m n_docs\n\u001B[0;32m    156\u001B[0m guesses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_scores2guesses(docs, scores)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\model.py:315\u001B[0m, in \u001B[0;36mModel.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m OutT:\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001B[39;00m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001B[39;00m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 315\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py:55\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     53\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m---> 55\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[0;32m     57\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\model.py:291\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, X, is_train)\u001B[0m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:38\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, Xseq, is_train)\u001B[0m\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m0\u001B[39m](Xseq, is_train)\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Tuple[SeqT, Callable], \u001B[43m_list_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXseq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:73\u001B[0m, in \u001B[0;36m_list_forward\u001B[1;34m(model, Xs, is_train)\u001B[0m\n\u001B[0;32m     71\u001B[0m lengths \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39masarray1i([\u001B[38;5;28mlen\u001B[39m(seq) \u001B[38;5;28;01mfor\u001B[39;00m seq \u001B[38;5;129;01min\u001B[39;00m Xs])\n\u001B[0;32m     72\u001B[0m Xf \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mflatten(Xs, pad\u001B[38;5;241m=\u001B[39mpad)\n\u001B[1;32m---> 73\u001B[0m Yf, get_dXf \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackprop\u001B[39m(dYs: ListXd) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ListXd:\n\u001B[0;32m     76\u001B[0m     dYf \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mflatten(dYs, pad\u001B[38;5;241m=\u001B[39mpad)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\model.py:291\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, X, is_train)\u001B[0m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\layers\\softmax.py:64\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     62\u001B[0m W \u001B[38;5;241m=\u001B[39m cast(Floats2d, model\u001B[38;5;241m.\u001B[39mget_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mW\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     63\u001B[0m b \u001B[38;5;241m=\u001B[39m cast(Floats1d, model\u001B[38;5;241m.\u001B[39mget_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m---> 64\u001B[0m Y \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maffine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m normalize:\n\u001B[0;32m     67\u001B[0m     Y \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39msoftmax(Y, temperature\u001B[38;5;241m=\u001B[39mtemperature)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\backends\\ops.py:228\u001B[0m, in \u001B[0;36mOps.affine\u001B[1;34m(self, X, W, b)\u001B[0m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21maffine\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: Floats2d, W: Floats2d, b: Floats1d) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Floats2d:\n\u001B[0;32m    225\u001B[0m     \u001B[38;5;124;03m\"\"\"Apply a weights layer and a bias to some inputs, i.e.\u001B[39;00m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;124;03m    Y = X @ W.T + b\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 228\u001B[0m     Y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgemm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    229\u001B[0m     Y \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m b\n\u001B[0;32m    230\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Y\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "test = [word for text in df_Lied['Text'] for word in nlp(text).ents]\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "titel_list = [word.upper() for word in list(set(df_Lied['TITEL']))]\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_list = [word for titel in titel_list for word in tokenizer.tokenize(titel)]\n",
    "no_stopwords_title_list = [item for item in tokenized_list if item not in stopwords_list]\n",
    "df_title_word_occurrence = pd.value_counts(np.array(no_stopwords_title_list))\n",
    "df_title_word_occurrence.to_csv('Data/processed/title_word_occurrence.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonal determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted Words -Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.merge(df_Lied, df_Chart_Position.set_axis(['DATUM_VON','DATUM_BIS', 'ID', 'POSITION', 'DAUER', 'BELIEBTHEIT', 'Jahr', 'Monat'], axis=1, inplace=False), on='ID', how='inner').sort_values(by=['ID'])\n",
    "print(df_Lied.isnull().values.any())#df muss noch ausgetauscht werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
