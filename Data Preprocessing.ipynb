{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from profanity import profanity\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "RAW_DATA_PATH = 'data/raw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "def find_longest_word(word_list):\n",
    "    longest_word =  max(word_list, key=len)\n",
    "    return len(longest_word)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "def lemmatize_string(input_text):\n",
    "    doc = nlp(input_text.lower())\n",
    "    result = ' '.join([x.lemma_ for x in doc])\n",
    "    doc = nlp(result.title())\n",
    "    result = ' '.join([x.lemma_ for x in doc]).upper()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID INTERPRET    TITEL SPRACHE_DEUTSCH  \\\n",
      "0  1148163     Yasha  Strand               Ja   \n",
      "1  1148163     Yasha  Strand               Ja   \n",
      "2  1148163     Yasha  Strand               Ja   \n",
      "3  1148163     Yasha  Strand               Ja   \n",
      "4  1148163     Yasha  Strand               Ja   \n",
      "\n",
      "                                          TEXT_TEIL1 TEXT_TEIL2 TEXT_TEIL3  \\\n",
      "0  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "1  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "2  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "3  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "4  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...        NaN        NaN   \n",
      "\n",
      "  TEXT_TEIL4  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n",
      "   LIED_ID  POSITION   DATUM_VON   DATUM_BIS\n",
      "0    26024        26  1983-04-18  1983-04-24\n",
      "1     1073        46  1984-06-04  1984-06-10\n",
      "2     1131        33  1984-06-18  1984-06-24\n",
      "3    15819        22  1984-08-13  1984-08-19\n",
      "4     7324         6  1984-08-27  1984-09-02\n"
     ]
    }
   ],
   "source": [
    "df_Lied = pd.read_csv(RAW_DATA_PATH + 'LIED.csv', usecols=['ID','INTERPRET', 'TITEL', 'SPRACHE_DEUTSCH', 'TEXT_TEIL1', 'TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4'])\n",
    "print(df_Lied.head())\n",
    "\n",
    "df_Chart_Position = pd.read_csv(RAW_DATA_PATH + 'CHART_POSITION.csv', usecols=['LIED_ID', 'POSITION', 'DATUM_VON', 'DATUM_BIS'])\n",
    "print(df_Chart_Position.head())\n",
    "\n",
    "with open(RAW_DATA_PATH+'Stoppwords.csv', newline='', encoding='UTF-8') as f:\n",
    "    stopwords_list = list(csv.reader(f))\n",
    "stopwords_list = [word.upper() for word in flatten(stopwords_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Lied['Text'] = df_Lied['TEXT_TEIL1'].fillna('') + df_Lied['TEXT_TEIL2'].fillna('') + df_Lied['TEXT_TEIL3'].fillna('') + df_Lied['TEXT_TEIL4'].fillna('')\n",
    "#df_Lied.drop(columns=['TEXT_TEIL1', 'TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4'], axis=1, inplace=True)\n",
    "\n",
    "df_Chart_Position['DATUM_VON'] = pd.to_datetime(df_Chart_Position['DATUM_VON'])\n",
    "df_Chart_Position['DATUM_BIS'] = pd.to_datetime(df_Chart_Position['DATUM_BIS'])\n",
    "df_Chart_Position['DAUER'] = (df_Chart_Position['DATUM_BIS'] - df_Chart_Position['DATUM_VON']).dt.days.astype('int16')\n",
    "df_Chart_Position['Jahr'] = df_Chart_Position['DATUM_BIS'].dt.year.astype('int16')\n",
    "df_Chart_Position['Monat'] =  df_Chart_Position['DATUM_BIS'].dt.month.astype('int16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...\n",
      "1    [LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...\n",
      "2    [LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...\n",
      "3    [LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...\n",
      "4    [LASS, STRAND, WEISST, BRAUCH, MAL, WIEDER, ME...\n",
      "Name: processed_Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_Lied['processed_Text'] = df_Lied['Text']\n",
    "\n",
    "#lemmatization\n",
    "df_Lied['processed_Text'] = df_Lied.processed_Text.apply(lambda text: lemmatize_string(text))\n",
    "\n",
    "#Stemming???\n",
    "#df_Lied['processed_Text'] = df_Lied.processed_Text.apply(lambda text: lemmatize_string(text))\n",
    "\n",
    "# tokenize\n",
    "df_Lied['processed_Text'] = df_Lied.processed_Text.apply(lambda text: nltk.word_tokenize(text))\n",
    "\n",
    "#remove stopwords\n",
    "df_Lied['processed_Text'] = df_Lied['processed_Text'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "\n",
    "#remove numbers\n",
    "df_Lied['processed_Text'] = df_Lied['processed_Text'].apply(lambda word_list : [re.sub('\\w*\\d\\w*','', word) for word in word_list])\n",
    "\n",
    "# ngram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LASS', 'STRAND', 'WEISST', 'BRAUCH', 'MAL', 'WIEDER', 'MEER', 'SAND', 'YEAH', 'BRAUCH', 'MAL', 'WIEDERGRUND', 'BLEIBEN', 'AI', 'UHR', 'FAELLT', 'BLAU', 'SEHEN', 'VERSINKT', 'SETZTEN', 'LANGSAM', 'WASSER', 'SPIEGELGLATT', 'BLINKT', 'WOLLEN', 'SEHNSUCHT', 'MORGEN', 'HEUTE', 'PERFEKT', 'IS', 'SEHNSUCHT', 'MORGEN', 'HEUTE', 'PERFEKT', 'IS', 'STEHST', 'ZEIGST', 'ECHT', 'LICHTER', 'STADT', 'VERBRANNT', 'LASS', 'STRAND', 'KOMM', 'STEIGEN', 'LEUCHTEST', 'DIAMANT', 'YEHE', 'ALL', 'SALZ', 'HAUT', 'AI', 'SEH', 'GESTOCHEN', 'SCHARF', 'TUT', 'FAST', 'AUGEN', 'WEH', 'ENDLICH', 'NOCH', 'NIE', 'GESEHEN', 'YEHI', 'WOLLEN', 'SEHNSUCHT', 'MORGEN', 'HEUTE', 'PERFEKT', 'IS', 'SEHNSUCHT', 'MORGEN', 'HEUTE', 'PERFEKT', 'IS', 'STEHST', 'ZEIGST', 'ECHT', 'LICHTER', 'STADT', 'VERBRANNT', 'LASS', 'STRAND', 'LASS', 'STRAND', 'WEISST', 'LASS', 'STRAND', 'YEAH', 'BRAUCHGRUND', 'BLEIBEN', 'AI', 'LASS', 'STRAND', 'WEISST', 'BRAUCH', 'MAL', 'WIEDER', 'MEER', 'SAND', 'YEAH', 'BRAUCH', 'MAL', 'WIEDERGRUND', 'BLEIBEN', 'OUH', 'LASS', 'STRAND']\n"
     ]
    }
   ],
   "source": [
    "print(df_Lied['processed_Text'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN ICH BRAUCH MAL WIEDER MEER UND SAND  YEAH BRAUCH MAL WIEDERGRUND ZU BLEIBEN  AI DEINE UHR FAELLT INS BLAU WIR SEHEN  WIE DIE ZEIT VERSINKT WIR SETZTEN LANGSAM AUF DAS WASSER SPIEGELGLATT UND BLINKT  WIR WOLLEN DIE SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS DU STEHST NEBEN MIR  ZEIGST MIR  DASS ES ECHT IST DIE LICHTER MEINER STADT SIND VERBRANNT LASS UNS ZUM STRAND    KOMM WIR STEIGEN AUS DU LEUCHTEST WIE AUS DIAMANT  YEHE MIT ALL DEM SALZ AUF DER HAUT  AI ICH SEH DICH GESTOCHEN SCHARF ES TUT FAST DEN AUGEN WEH UND ENDLICH BIST DU DA SO HABE ICH DICH NOCH NIE GESEHEN  YEHI  WIR WOLLEN DIE SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS DU STEHST NEBEN MIR  ZEIGST MIR  DASS ES ECHT IST DIE LICHTER MEINER STADT SIND VERBRANNT LASS UNS ZUM STRAND   LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN LASS UNS ZUM STRAND  YEAH ICH BRAUCHGRUND ZU BLEIBEN  AI LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN ICH BRAUCH MAL WIEDER MEER UND SAND  YEAH BRAUCH MAL WIEDERGRUND ZU BLEIBEN  OUH LASS UNS ZUM STRAND \n"
     ]
    }
   ],
   "source": [
    "print(df_Lied['Text'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokenized_text'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3802\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3803\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'tokenized_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [79], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdf_Lied\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtokenized_text\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;241m1000\u001B[39m])\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:3804\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3802\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3803\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3804\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3806\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3803\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3805\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3808\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3809\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3810\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'tokenized_text'"
     ]
    }
   ],
   "source": [
    "print(df_Lied['tokenized_text'][1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df_Lied['TEXT_LAENGE'] = list(len(word) for word in df_Lied['Text'])\n",
    "df_Lied['ANZ_UNIQUE_WOERTER'] = list(len(set(word.split(\" \"))) for word in df_Lied['Text'])\n",
    "df_Lied['LAENGE_LAENGSTES_WORT'] = list(len(max(set(word.split(\" \")), key=len)) for word in df_Lied['Text'])\n",
    "df_Lied[['LAENGE_LAENGSTES_WORT','ID']].to_csv('Data/processed/Wort_LAENGE_v2.csv')\n",
    "df_Lied['INTERPRET'].value_counts(ascending=True, dropna=True, normalize=True)\\\n",
    "    #.to_csv('Data/processed/Interpret_Occurrences_normalised.csv')\n",
    "\n",
    "MAX_RANK = 50\n",
    "df_Chart_Position['RANK_SCORE'] = MAX_RANK - df_Chart_Position['POSITION'] + 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID INTERPRET    TITEL  \\\n",
      "0  1148163     Yasha  Strand    \n",
      "1  1148163     Yasha  Strand    \n",
      "2  1148163     Yasha  Strand    \n",
      "3  1148163     Yasha  Strand    \n",
      "4  1148163     Yasha  Strand    \n",
      "\n",
      "                                                Text  ANZ_UNIQUE_WOERTER  \\\n",
      "0  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...                  92   \n",
      "1  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...                  92   \n",
      "2  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...                  92   \n",
      "3  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...                  92   \n",
      "4  LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN...                  92   \n",
      "\n",
      "   LAENGE_LAENGSTES_WORT  POSITION  DATUM_VON  DATUM_BIS  DAUER  Jahr  Monat  \\\n",
      "0                     12        47 2013-08-23 2013-08-29      6  2013      8   \n",
      "1                     12        19 2013-06-14 2013-06-20      6  2013      6   \n",
      "2                     12        33 2013-07-05 2013-07-11      6  2013      7   \n",
      "3                     12        41 2013-07-26 2013-08-01      6  2013      8   \n",
      "4                     12        22 2013-06-28 2013-07-04      6  2013      7   \n",
      "\n",
      "   RANK_SCORE  \n",
      "0           4  \n",
      "1          32  \n",
      "2          18  \n",
      "3          10  \n",
      "4          29  \n",
      "Index(['ID', 'INTERPRET', 'TITEL', 'Text', 'ANZ_UNIQUE_WOERTER',\n",
      "       'LAENGE_LAENGSTES_WORT'],\n",
      "      dtype='object')\n",
      "Index(['LIED_ID', 'POSITION', 'DATUM_VON', 'DATUM_BIS', 'DAUER', 'Jahr',\n",
      "       'Monat', 'RANK_SCORE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_Lied.drop(['TEXT_TEIL1','TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4'], axis=1, inplace=True)\n",
    "df_Lied.drop(['SPRACHE_DEUTSCH'], axis=1, inplace=True)\n",
    "df_Chart_Position.rename(columns={'ID':'Lied_ID'}, inplace=True)\n",
    "df = df_Lied.merge(df_Chart_Position.rename(columns={'LIED_ID':'ID'}),on='ID', how='inner')\n",
    "print(df.head())\n",
    "print(df_Lied.columns)\n",
    "print(df_Chart_Position.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "profanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RAW_DATA_PATH+'german-profanity-list.csv', newline='', encoding='UTF-8') as f:\n",
    "    profanity_list = list(csv.reader(f))\n",
    "\n",
    "profanity.load_words(flatten(profanity_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Percentage of Stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "#df['Number_of_Stopwords'] = df['Text'].str.split().apply(lambda x: len(set(x) & set(stopwords_list)))\n",
    "#df['Stopword_Percentage'] = df.Number_of_Stopwords.apply(lambda row: row/len(df['Text']))\n",
    "df_Lied[['ID','Number_of_Stopwords','Stopword_Percentage', 'RANK_SCORE']].to_csv('Data/processed/Stoppwords-RANK_SCORE.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'processed_text'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [118], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#df_Lied['SUBJEKTIVITY'] = df_Lied.processed_text.apply(lambda text: TextBlob(str(text)).sentiment.subjectivity)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m df_Lied[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPOLARITY\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf_Lied\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocessed_text\u001B[49m\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m text: TextBlob(\u001B[38;5;28mstr\u001B[39m(text))\u001B[38;5;241m.\u001B[39msentiment\u001B[38;5;241m.\u001B[39mpolarity)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:5902\u001B[0m, in \u001B[0;36mNDFrame.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   5895\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   5896\u001B[0m     name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_internal_names_set\n\u001B[0;32m   5897\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata\n\u001B[0;32m   5898\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accessors\n\u001B[0;32m   5899\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_axis\u001B[38;5;241m.\u001B[39m_can_hold_identifiers_and_holds_name(name)\n\u001B[0;32m   5900\u001B[0m ):\n\u001B[0;32m   5901\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[name]\n\u001B[1;32m-> 5902\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'processed_text'"
     ]
    }
   ],
   "source": [
    "#df_Lied['SUBJEKTIVITY'] = df_Lied.processed_text.apply(lambda text: TextBlob(str(text)).sentiment.subjectivity)\n",
    "df_Lied['POLARITY'] = df_Lied.processed_text.apply(lambda text: TextBlob(str(text)).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_Lied[['ID','Number_of_Stopwords','Stopword_Percentage', 'RANK_SCORE']].to_csv('Data/processed/Sentiment.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA (BoW) - Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.2, subjectivity=0.6)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [32], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m test \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[43mword\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdf_Lied\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mText\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mword\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ments\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(test[\u001B[38;5;241m0\u001B[39m])\n",
      "Cell \u001B[1;32mIn [32], line 1\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m test \u001B[38;5;241m=\u001B[39m [word \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m df_Lied[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mText\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39ments]\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(test[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\spacy\\language.py:1026\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[1;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[0;32m   1024\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[0;32m   1025\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1026\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1028\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[0;32m   1029\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001B[0m, in \u001B[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\spacy\\pipeline\\edit_tree_lemmatizer.py:154\u001B[0m, in \u001B[0;36mEditTreeLemmatizer.predict\u001B[1;34m(self, docs)\u001B[0m\n\u001B[0;32m    152\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(guesses) \u001B[38;5;241m==\u001B[39m n_docs\n\u001B[0;32m    153\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m guesses\n\u001B[1;32m--> 154\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(scores) \u001B[38;5;241m==\u001B[39m n_docs\n\u001B[0;32m    156\u001B[0m guesses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_scores2guesses(docs, scores)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\model.py:315\u001B[0m, in \u001B[0;36mModel.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m OutT:\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001B[39;00m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001B[39;00m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 315\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py:55\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     53\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m---> 55\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[0;32m     57\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\model.py:291\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, X, is_train)\u001B[0m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:38\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, Xseq, is_train)\u001B[0m\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m0\u001B[39m](Xseq, is_train)\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Tuple[SeqT, Callable], \u001B[43m_list_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXseq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:73\u001B[0m, in \u001B[0;36m_list_forward\u001B[1;34m(model, Xs, is_train)\u001B[0m\n\u001B[0;32m     71\u001B[0m lengths \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39masarray1i([\u001B[38;5;28mlen\u001B[39m(seq) \u001B[38;5;28;01mfor\u001B[39;00m seq \u001B[38;5;129;01min\u001B[39;00m Xs])\n\u001B[0;32m     72\u001B[0m Xf \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mflatten(Xs, pad\u001B[38;5;241m=\u001B[39mpad)\n\u001B[1;32m---> 73\u001B[0m Yf, get_dXf \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackprop\u001B[39m(dYs: ListXd) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ListXd:\n\u001B[0;32m     76\u001B[0m     dYf \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mflatten(dYs, pad\u001B[38;5;241m=\u001B[39mpad)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\model.py:291\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, X, is_train)\u001B[0m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\layers\\softmax.py:64\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     62\u001B[0m W \u001B[38;5;241m=\u001B[39m cast(Floats2d, model\u001B[38;5;241m.\u001B[39mget_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mW\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     63\u001B[0m b \u001B[38;5;241m=\u001B[39m cast(Floats1d, model\u001B[38;5;241m.\u001B[39mget_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m---> 64\u001B[0m Y \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maffine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m normalize:\n\u001B[0;32m     67\u001B[0m     Y \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39msoftmax(Y, temperature\u001B[38;5;241m=\u001B[39mtemperature)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\backends\\ops.py:228\u001B[0m, in \u001B[0;36mOps.affine\u001B[1;34m(self, X, W, b)\u001B[0m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21maffine\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: Floats2d, W: Floats2d, b: Floats1d) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Floats2d:\n\u001B[0;32m    225\u001B[0m     \u001B[38;5;124;03m\"\"\"Apply a weights layer and a bias to some inputs, i.e.\u001B[39;00m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;124;03m    Y = X @ W.T + b\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 228\u001B[0m     Y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgemm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    229\u001B[0m     Y \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m b\n\u001B[0;32m    230\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Y\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "test = [word for text in df_Lied['Text'] for word in nlp(text).ents]\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "titel_list = [word.upper() for word in list(set(df_Lied['TITEL']))]\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_list = [word for titel in titel_list for word in tokenizer.tokenize(titel)]\n",
    "no_stopwords_title_list = [item for item in tokenized_list if item not in stopwords_list]\n",
    "df_title_word_occurrence = pd.value_counts(np.array(no_stopwords_title_list))\n",
    "df_title_word_occurrence.to_csv('Data/processed/title_word_occurrence.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonal determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted Words -Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_Lied, df_Chart_Position.set_axis(['DATUM_VON','DATUM_BIS', 'ID', 'POSITION', 'DAUER', 'BELIEBTHEIT', 'Jahr', 'Monat'], axis=1, inplace=False), on='ID', how='inner').sort_values(by=['ID'])\n",
    "#print(df_Lied.isnull().values.any())#df muss noch ausgetauscht werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
