{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pydev debugger: Unable to find real location for: C:\\Users\\Bennet GÃ¶ttsche\\AppData\\Local\\Temp\\ipykernel_44608\\2459993205.py\n",
      "pydev debugger: Unable to find real location for: <frozen importlib._bootstrap_external>\n",
      "pydev debugger: Unable to find real location for: <frozen codecs>\n",
      "pydev debugger: Unable to find real location for: <frozen io>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from profanity import profanity\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "snowStemmer = SnowballStemmer(language='german')\n",
    "RAW_DATA_PATH = 'data/raw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def find_longest_word(word_list):\n",
    "    longest_word =  max(word_list, key=len)\n",
    "    return len(longest_word)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def lemmatize_string(input_text):\n",
    "    doc = nlp(input_text.lower())\n",
    "    result = ' '.join([x.lemma_ for x in doc])\n",
    "    doc = nlp(result.title())\n",
    "    result = ' '.join([x.lemma_ for x in doc]).upper()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LIED_ID  POSITION   DATUM_VON   DATUM_BIS\n",
      "0    26024        26  1983-04-18  1983-04-24\n",
      "1     1073        46  1984-06-04  1984-06-10\n",
      "2     1131        33  1984-06-18  1984-06-24\n",
      "3    15819        22  1984-08-13  1984-08-19\n",
      "4     7324         6  1984-08-27  1984-09-02\n"
     ]
    }
   ],
   "source": [
    "df_Lied = pd.read_csv(RAW_DATA_PATH + 'LIED.csv', usecols=['ID','INTERPRET', 'TITEL', 'SPRACHE_DEUTSCH', 'TEXT_TEIL1', 'TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4'])\n",
    "#print(df_Lied.head())\n",
    "\n",
    "df_Chart_Position = pd.read_csv(RAW_DATA_PATH + 'CHART_POSITION.csv', usecols=['LIED_ID', 'POSITION', 'DATUM_VON', 'DATUM_BIS'])\n",
    "print(df_Chart_Position.head())\n",
    "\n",
    "#get stopword-list\n",
    "with open(RAW_DATA_PATH+'Stoppwords.csv', newline='', encoding='UTF-8') as f:\n",
    "    stopwords_list = list(csv.reader(f))\n",
    "stopwords_list = [word.upper() for word in flatten(stopwords_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Lied['Text'] = df_Lied['TEXT_TEIL1'].fillna('') + df_Lied['TEXT_TEIL2'].fillna('') + df_Lied['TEXT_TEIL3'].fillna('') + df_Lied['TEXT_TEIL4'].fillna('')\n",
    "\n",
    "\n",
    "df_Chart_Position['DATUM_VON'] = pd.to_datetime(df_Chart_Position['DATUM_VON'])\n",
    "df_Chart_Position['DATUM_BIS'] = pd.to_datetime(df_Chart_Position['DATUM_BIS'])\n",
    "df_Chart_Position['DAUER'] = (df_Chart_Position['DATUM_BIS'] - df_Chart_Position['DATUM_VON']).dt.days.astype('int16')\n",
    "df_Chart_Position['Jahr'] = df_Chart_Position['DATUM_BIS'].dt.year.astype('int16')\n",
    "df_Chart_Position['Monat'] =  df_Chart_Position['DATUM_BIS'].dt.month.astype('int16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Lied['processed_Text'] = df_Lied['Text']\n",
    "\n",
    "# ngram\n",
    "\n",
    "#lemmatization\n",
    "df_Lied['processed_Text'] = df_Lied.processed_Text.apply(lambda text: lemmatize_string(text))\n",
    "\n",
    "#Stemming\n",
    "#df_Lied['processed_Text'] = df_Lied.processed_Text.apply(lambda text: [snowStemmer.stem(token) for token in text])\n",
    "\n",
    "# tokenize\n",
    "df_Lied['processed_Text'] = df_Lied.processed_Text.apply(lambda text: nltk.word_tokenize(text))\n",
    "\n",
    "#remove stopwords\n",
    "df_Lied['processed_Text'] = df_Lied['processed_Text'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "\n",
    "#remove numbers\n",
    "df_Lied['processed_Text'] = df_Lied['processed_Text'].apply(lambda word_list : [re.sub('\\w*\\d\\w*','', word) for word in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LASS', 'STRAND', 'WEISSEN', 'BRAUCH', 'MAL', 'WIEDER', 'MEER', 'SAND', 'YEAH', 'BRAUCH', 'MAL', 'WIEDERGRUND', 'BLEIBEN', 'AI', 'UHR', 'FAELLT', 'BLAU', 'SEHEN', 'VERSINKEN', 'SETZEN', 'LANGSAM', 'WASSER', 'SPIEGELGLATT', 'BLINKEN', 'WOLLEN', 'SEHNSUCHT', 'MORGEN', 'HEUTE', 'PERFEKT', '--', 'SEHNSUCHT', 'MORGEN', 'HEUTE', 'PERFEKT', '--', 'STEHST', 'ZEIGST', 'ECHT', 'LICHTER', 'STADT', 'VERBRANNT', 'LASS', 'STRAND', 'KOMMN', 'STEIGEN', 'LEUCHTEST', 'DIAMANT', 'YEHE', 'ALL', 'SALZ', 'HAUT', 'AI', 'SEH', 'GESTOCHEN', 'SCHARF', 'TUN', 'FAST', 'AUGE', 'WEH', 'ENDLICH', 'NOCH', 'NIE', 'SEHEN', 'YEHI', 'WOLLEN', 'SEHNSUCHT', 'MORGEN', 'HEUTE', 'PERFEKT', '--', 'SEHNSUCHT', 'MORGEN', 'HEUTE', 'PERFEKT', '--', 'STEHST', 'ZEIGST', 'ECHT', 'LICHTER', 'STADT', 'VERBRANNT', 'LASS', 'STRAND', 'LASS', 'STRAND', 'WEISSEN', 'LASS', 'STRAND', 'YEAH', 'BRAUCHGRUND', 'BLEIBEN', 'AI', 'LASS', 'STRAND', 'WEISSEN', 'BRAUCH', 'MAL', 'WIEDER', 'MEER', 'SAND', 'YEAH', 'BRAUCH', 'MAL', 'WIEDERGRUND', 'BLEIBEN', 'OUH', 'LASS', 'STRAND']\n"
     ]
    }
   ],
   "source": [
    "print(df_Lied['processed_Text'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN ICH BRAUCH MAL WIEDER MEER UND SAND  YEAH BRAUCH MAL WIEDERGRUND ZU BLEIBEN  AI DEINE UHR FAELLT INS BLAU WIR SEHEN  WIE DIE ZEIT VERSINKT WIR SETZTEN LANGSAM AUF DAS WASSER SPIEGELGLATT UND BLINKT  WIR WOLLEN DIE SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS DU STEHST NEBEN MIR  ZEIGST MIR  DASS ES ECHT IST DIE LICHTER MEINER STADT SIND VERBRANNT LASS UNS ZUM STRAND    KOMM WIR STEIGEN AUS DU LEUCHTEST WIE AUS DIAMANT  YEHE MIT ALL DEM SALZ AUF DER HAUT  AI ICH SEH DICH GESTOCHEN SCHARF ES TUT FAST DEN AUGEN WEH UND ENDLICH BIST DU DA SO HABE ICH DICH NOCH NIE GESEHEN  YEHI  WIR WOLLEN DIE SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS SEHNSUCHT NACH MORGEN  WEIL ES HEUTE PERFEKT IS DU STEHST NEBEN MIR  ZEIGST MIR  DASS ES ECHT IST DIE LICHTER MEINER STADT SIND VERBRANNT LASS UNS ZUM STRAND   LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN LASS UNS ZUM STRAND  YEAH ICH BRAUCHGRUND ZU BLEIBEN  AI LASS UNS ZUM STRAND DU WEISST WELCHEN ICH MEIN ICH BRAUCH MAL WIEDER MEER UND SAND  YEAH BRAUCH MAL WIEDERGRUND ZU BLEIBEN  OUH LASS UNS ZUM STRAND \n"
     ]
    }
   ],
   "source": [
    "print(df_Lied['Text'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID       INTERPRET   TITEL  \\\n",
      "7219  408     Rumpelstilz  Kiosk    \n",
      "7220  408     Rumpelstilz  Kiosk    \n",
      "7763  425  Costa Cordalis  Anita    \n",
      "7762  425  Costa Cordalis  Anita    \n",
      "7761  425  Costa Cordalis  Anita    \n",
      "\n",
      "                                                   Text  \\\n",
      "7219  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "7220  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "7763  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "7762  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "7761  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "\n",
      "                                         processed_Text  POSITION  DATUM_VON  \\\n",
      "7219  [ALSO, SAMMLE, FUER, GUT, ZWECK, SAGEN, FRITZ,...        50 2009-03-06   \n",
      "7220  [ALSO, SAMMLE, FUER, GUT, ZWECK, SAGEN, FRITZ,...         4 1984-10-22   \n",
      "7763  [JVUIOUGIVTOH, FINDEN, IRGENDWO, ALLEIN, MEXIK...        23 2003-08-25   \n",
      "7762  [JVUIOUGIVTOH, FINDEN, IRGENDWO, ALLEIN, MEXIK...        39 2003-07-28   \n",
      "7761  [JVUIOUGIVTOH, FINDEN, IRGENDWO, ALLEIN, MEXIK...        27 2003-07-28   \n",
      "\n",
      "      DATUM_BIS  DAUER  Jahr  Monat  \n",
      "7219 2009-03-12      6  2009      3  \n",
      "7220 1984-10-28      6  1984     10  \n",
      "7763 2003-08-31      6  2003      8  \n",
      "7762 2003-08-03      6  2003      8  \n",
      "7761 2003-08-03      6  2003      8  \n"
     ]
    }
   ],
   "source": [
    "df_Lied.drop(['TEXT_TEIL1','TEXT_TEIL2', 'TEXT_TEIL3', 'TEXT_TEIL4', 'SPRACHE_DEUTSCH'], axis=1, inplace=True)\n",
    "#df_Date = df_Chart_Position[['LIED_ID', 'DATUM_VON', 'DATUM_BIS', 'DAUER','Jahr', 'Monat']]\n",
    "#df_Chart_Position.drop(['DATUM_VON', 'DATUM_BIS', 'Jahr', 'Monat'], axis=1, inplace=True)\n",
    "\n",
    "#not used anymore:\n",
    "#df_Chart_Position = df_Chart_Position.groupby('LIED_ID').agg({'POSITION':'mean','DAUER':'sum'}).reset_index()\n",
    "#df_Chart_Position['POSITION'] = df_Chart_Position.POSITION.apply(lambda pos: round(pos))\n",
    "#----\n",
    "\n",
    "df_Lied.sort_values(by='ID', inplace=True)\n",
    "df_Chart_Position.sort_values(by='LIED_ID', inplace=True)\n",
    "df = pd.concat([df_Lied, df_Chart_Position], axis='columns')\n",
    "df.drop('LIED_ID', axis=1, inplace=True)\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df['ANZ_UNIQUE_WOERTER'] = list(len(set(word)) for word in df['processed_Text'])\n",
    "df['LAENGE_LAENGSTES_WORT'] = list(len(max(set(word), key=len)) for word in df['processed_Text'])\n",
    "\n",
    "MAX_RANK = 50\n",
    "df['RANK_SCORE'] = MAX_RANK - df['POSITION'] + 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "profanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RAW_DATA_PATH+'german-profanity-list.csv', newline='', encoding='UTF-8') as f:\n",
    "    profanity_list = list(csv.reader(f))\n",
    "\n",
    "profanity.load_words(flatten(profanity_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Percentage of Stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7219    0.00211\n",
      "7220    0.00211\n",
      "7763    0.00189\n",
      "7762    0.00189\n",
      "7761    0.00189\n",
      "7760    0.00189\n",
      "7743    0.00189\n",
      "7744    0.00189\n",
      "7745    0.00189\n",
      "7746    0.00189\n",
      "7747    0.00189\n",
      "7748    0.00189\n",
      "7749    0.00189\n",
      "7751    0.00189\n",
      "7752    0.00189\n",
      "7753    0.00189\n",
      "7754    0.00189\n",
      "7755    0.00189\n",
      "7756    0.00189\n",
      "7757    0.00189\n",
      "Name: Stopword_Percentage, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df['Number_of_Stopwords'] = df.Text.str.split().apply(lambda x: len(set(x) & set(stopwords_list)))\n",
    "df['Stopword_Percentage'] = df.Number_of_Stopwords.apply(lambda row: round(row/len(df['Text']), ndigits=5))\n",
    "print(df['Stopword_Percentage'].head(20))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SUBJEKTIVITY'] = df.processed_Text.apply(lambda text: TextBlob(str(text)).sentiment.subjectivity)\n",
    "df['POLARITY'] = df.processed_Text.apply(lambda text: TextBlob(str(text)).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID       INTERPRET   TITEL  \\\n",
      "0  408     Rumpelstilz  Kiosk    \n",
      "1  408     Rumpelstilz  Kiosk    \n",
      "2  425  Costa Cordalis  Anita    \n",
      "3  425  Costa Cordalis  Anita    \n",
      "4  425  Costa Cordalis  Anita    \n",
      "\n",
      "                                                Text  \\\n",
      "0  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "1  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n",
      "2  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "3  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "4  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n",
      "\n",
      "                                      processed_Text  POSITION  DATUM_VON  \\\n",
      "0  [ALSO, SAMMLE, FUER, GUT, ZWECK, SAGEN, FRITZ,...        50 2009-03-06   \n",
      "1  [ALSO, SAMMLE, FUER, GUT, ZWECK, SAGEN, FRITZ,...         4 1984-10-22   \n",
      "2  [JVUIOUGIVTOH, FINDEN, IRGENDWO, ALLEIN, MEXIK...        23 2003-08-25   \n",
      "3  [JVUIOUGIVTOH, FINDEN, IRGENDWO, ALLEIN, MEXIK...        39 2003-07-28   \n",
      "4  [JVUIOUGIVTOH, FINDEN, IRGENDWO, ALLEIN, MEXIK...        27 2003-07-28   \n",
      "\n",
      "   DATUM_BIS  DAUER  Jahr  Monat  ANZ_UNIQUE_WOERTER  LAENGE_LAENGSTES_WORT  \\\n",
      "0 2009-03-12      6  2009      3                  82                     15   \n",
      "1 1984-10-28      6  1984     10                  82                     15   \n",
      "2 2003-08-31      6  2003      8                  74                     12   \n",
      "3 2003-08-03      6  2003      8                  74                     12   \n",
      "4 2003-08-03      6  2003      8                  74                     12   \n",
      "\n",
      "   RANK_SCORE  Number_of_Stopwords  Stopword_Percentage  SUBJEKTIVITY  \\\n",
      "0           1                   48              0.00211           0.4   \n",
      "1          47                   48              0.00211           0.4   \n",
      "2          28                   43              0.00189           0.0   \n",
      "3          12                   43              0.00189           0.0   \n",
      "4          24                   43              0.00189           0.0   \n",
      "\n",
      "   POLARITY  \n",
      "0       0.1  \n",
      "1       0.1  \n",
      "2       0.0  \n",
      "3       0.0  \n",
      "4       0.0  \n"
     ]
    }
   ],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.drop('index',axis=1, inplace=True)\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA (BoW) - Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [32], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m test \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[43mword\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdf_Lied\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mText\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mword\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ments\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(test[\u001B[38;5;241m0\u001B[39m])\n",
      "Cell \u001B[1;32mIn [32], line 1\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m test \u001B[38;5;241m=\u001B[39m [word \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m df_Lied[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mText\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39ments]\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(test[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\spacy\\language.py:1026\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[1;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[0;32m   1024\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[0;32m   1025\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1026\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1028\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[0;32m   1029\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001B[0m, in \u001B[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\spacy\\pipeline\\edit_tree_lemmatizer.py:154\u001B[0m, in \u001B[0;36mEditTreeLemmatizer.predict\u001B[1;34m(self, docs)\u001B[0m\n\u001B[0;32m    152\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(guesses) \u001B[38;5;241m==\u001B[39m n_docs\n\u001B[0;32m    153\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m guesses\n\u001B[1;32m--> 154\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(scores) \u001B[38;5;241m==\u001B[39m n_docs\n\u001B[0;32m    156\u001B[0m guesses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_scores2guesses(docs, scores)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\model.py:315\u001B[0m, in \u001B[0;36mModel.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m OutT:\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001B[39;00m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001B[39;00m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 315\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py:55\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     53\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m---> 55\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[0;32m     57\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\model.py:291\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, X, is_train)\u001B[0m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:38\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, Xseq, is_train)\u001B[0m\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m0\u001B[39m](Xseq, is_train)\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Tuple[SeqT, Callable], \u001B[43m_list_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXseq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:73\u001B[0m, in \u001B[0;36m_list_forward\u001B[1;34m(model, Xs, is_train)\u001B[0m\n\u001B[0;32m     71\u001B[0m lengths \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39masarray1i([\u001B[38;5;28mlen\u001B[39m(seq) \u001B[38;5;28;01mfor\u001B[39;00m seq \u001B[38;5;129;01min\u001B[39;00m Xs])\n\u001B[0;32m     72\u001B[0m Xf \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mflatten(Xs, pad\u001B[38;5;241m=\u001B[39mpad)\n\u001B[1;32m---> 73\u001B[0m Yf, get_dXf \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackprop\u001B[39m(dYs: ListXd) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ListXd:\n\u001B[0;32m     76\u001B[0m     dYf \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mflatten(dYs, pad\u001B[38;5;241m=\u001B[39mpad)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\model.py:291\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, X, is_train)\u001B[0m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\layers\\softmax.py:64\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     62\u001B[0m W \u001B[38;5;241m=\u001B[39m cast(Floats2d, model\u001B[38;5;241m.\u001B[39mget_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mW\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     63\u001B[0m b \u001B[38;5;241m=\u001B[39m cast(Floats1d, model\u001B[38;5;241m.\u001B[39mget_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m---> 64\u001B[0m Y \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maffine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m normalize:\n\u001B[0;32m     67\u001B[0m     Y \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39msoftmax(Y, temperature\u001B[38;5;241m=\u001B[39mtemperature)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.2\\projects\\workspace\\venv\\Lib\\site-packages\\thinc\\backends\\ops.py:228\u001B[0m, in \u001B[0;36mOps.affine\u001B[1;34m(self, X, W, b)\u001B[0m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21maffine\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: Floats2d, W: Floats2d, b: Floats1d) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Floats2d:\n\u001B[0;32m    225\u001B[0m     \u001B[38;5;124;03m\"\"\"Apply a weights layer and a bias to some inputs, i.e.\u001B[39;00m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;124;03m    Y = X @ W.T + b\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 228\u001B[0m     Y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgemm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    229\u001B[0m     Y \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m b\n\u001B[0;32m    230\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Y\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "test = [word for text in df_Lied['Text'] for word in nlp(text).ents]\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "    ID       INTERPRET   TITEL  \\\n0  408     Rumpelstilz  Kiosk    \n1  408     Rumpelstilz  Kiosk    \n2  425  Costa Cordalis  Anita    \n3  425  Costa Cordalis  Anita    \n4  425  Costa Cordalis  Anita    \n\n                                                Text  \\\n0  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n1  ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...   \n2  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n3  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n4  JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...   \n\n                                      processed_Text  POSITION  DATUM_VON  \\\n0  [ALSO, SAMMLE, FUER, GUT, ZWECK, SAGEN, FRITZ,...        50 2009-03-06   \n1  [ALSO, SAMMLE, FUER, GUT, ZWECK, SAGEN, FRITZ,...         4 1984-10-22   \n2  [JVUIOUGIVTOH, FINDEN, IRGENDWO, ALLEIN, MEXIK...        23 2003-08-25   \n3  [JVUIOUGIVTOH, FINDEN, IRGENDWO, ALLEIN, MEXIK...        39 2003-07-28   \n4  [JVUIOUGIVTOH, FINDEN, IRGENDWO, ALLEIN, MEXIK...        27 2003-07-28   \n\n   DATUM_BIS  DAUER  Jahr  Monat  ANZ_UNIQUE_WOERTER  LAENGE_LAENGSTES_WORT  \\\n0 2009-03-12      6  2009      3                  82                     15   \n1 1984-10-28      6  1984     10                  82                     15   \n2 2003-08-31      6  2003      8                  74                     12   \n3 2003-08-03      6  2003      8                  74                     12   \n4 2003-08-03      6  2003      8                  74                     12   \n\n   RANK_SCORE  Number_of_Stopwords  Stopword_Percentage  SUBJEKTIVITY  \\\n0           1                   48              0.00211           0.4   \n1          47                   48              0.00211           0.4   \n2          28                   43              0.00189           0.0   \n3          12                   43              0.00189           0.0   \n4          24                   43              0.00189           0.0   \n\n   POLARITY  \n0       0.1  \n1       0.1  \n2       0.0  \n3       0.0  \n4       0.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>INTERPRET</th>\n      <th>TITEL</th>\n      <th>Text</th>\n      <th>processed_Text</th>\n      <th>POSITION</th>\n      <th>DATUM_VON</th>\n      <th>DATUM_BIS</th>\n      <th>DAUER</th>\n      <th>Jahr</th>\n      <th>Monat</th>\n      <th>ANZ_UNIQUE_WOERTER</th>\n      <th>LAENGE_LAENGSTES_WORT</th>\n      <th>RANK_SCORE</th>\n      <th>Number_of_Stopwords</th>\n      <th>Stopword_Percentage</th>\n      <th>SUBJEKTIVITY</th>\n      <th>POLARITY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>408</td>\n      <td>Rumpelstilz</td>\n      <td>Kiosk</td>\n      <td>ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...</td>\n      <td>[ALSO, SAMMLE, FUER, GUT, ZWECK, SAGEN, FRITZ,...</td>\n      <td>50</td>\n      <td>2009-03-06</td>\n      <td>2009-03-12</td>\n      <td>6</td>\n      <td>2009</td>\n      <td>3</td>\n      <td>82</td>\n      <td>15</td>\n      <td>1</td>\n      <td>48</td>\n      <td>0.00211</td>\n      <td>0.4</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>408</td>\n      <td>Rumpelstilz</td>\n      <td>Kiosk</td>\n      <td>ALSO ER SAMMLE FUER EINEN GUTEN ZWECK  SAGT DE...</td>\n      <td>[ALSO, SAMMLE, FUER, GUT, ZWECK, SAGEN, FRITZ,...</td>\n      <td>4</td>\n      <td>1984-10-22</td>\n      <td>1984-10-28</td>\n      <td>6</td>\n      <td>1984</td>\n      <td>10</td>\n      <td>82</td>\n      <td>15</td>\n      <td>47</td>\n      <td>48</td>\n      <td>0.00211</td>\n      <td>0.4</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>425</td>\n      <td>Costa Cordalis</td>\n      <td>Anita</td>\n      <td>JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...</td>\n      <td>[JVUIOUGIVTOH, FINDEN, IRGENDWO, ALLEIN, MEXIK...</td>\n      <td>23</td>\n      <td>2003-08-25</td>\n      <td>2003-08-31</td>\n      <td>6</td>\n      <td>2003</td>\n      <td>8</td>\n      <td>74</td>\n      <td>12</td>\n      <td>28</td>\n      <td>43</td>\n      <td>0.00189</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>425</td>\n      <td>Costa Cordalis</td>\n      <td>Anita</td>\n      <td>JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...</td>\n      <td>[JVUIOUGIVTOH, FINDEN, IRGENDWO, ALLEIN, MEXIK...</td>\n      <td>39</td>\n      <td>2003-07-28</td>\n      <td>2003-08-03</td>\n      <td>6</td>\n      <td>2003</td>\n      <td>8</td>\n      <td>74</td>\n      <td>12</td>\n      <td>12</td>\n      <td>43</td>\n      <td>0.00189</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>425</td>\n      <td>Costa Cordalis</td>\n      <td>Anita</td>\n      <td>JVUIOUGIVTOH ICH FAND SIE IRGENDWO  ALLEIN IN ...</td>\n      <td>[JVUIOUGIVTOH, FINDEN, IRGENDWO, ALLEIN, MEXIK...</td>\n      <td>27</td>\n      <td>2003-07-28</td>\n      <td>2003-08-03</td>\n      <td>6</td>\n      <td>2003</td>\n      <td>8</td>\n      <td>74</td>\n      <td>12</td>\n      <td>24</td>\n      <td>43</td>\n      <td>0.00189</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Kiosk\n",
      "1    Kiosk\n",
      "2    Anita\n",
      "3    Anita\n",
      "4    Anita\n",
      "Name: processed_Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#titel_list = [word.upper() for word in list(set(df['TITEL']))]\n",
    "#print(titel_list)\n",
    "#lemmatize title\n",
    "df['processed_Title'] = df.TITEL.apply(lambda titel: ' '.join([x.lemma_ for x in nlp(titel)]))\n",
    "print(df['processed_Title'].head())\n",
    "\n",
    "#tokenize title\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['processed_Title'] = df.processed_Title.apply(lambda titel: [word for word in tokenizer.tokenize(titel)])\n",
    "\n",
    "#remove stopwords\n",
    "df['processed_Title'] = df.processed_Title.apply(lambda titel: [word for word in titel if word.upper() not in stopwords_list])\n",
    "\n",
    "#remove numbers\n",
    "df['processed_Title'] = df['processed_Title'].apply(lambda word_list : [re.sub('\\w*\\d\\w*','', word) for word in word_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonal determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                  [Kiosk]\n",
      "1                                  [Kiosk]\n",
      "2                                  [Anita]\n",
      "3                                  [Anita]\n",
      "4                                  [Anita]\n",
      "                      ...                 \n",
      "65                    [Mond, blÃ¼hen, Rose]\n",
      "66                    [Mond, blÃ¼hen, Rose]\n",
      "67    [MuÃt, jetzt, grade, gehen, Lucille]\n",
      "68    [MuÃt, jetzt, grade, gehen, Lucille]\n",
      "69    [MuÃt, jetzt, grade, gehen, Lucille]\n",
      "Name: processed_Title, Length: 70, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['processed_Title'].head(70))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted Words -Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create csv\n",
    "#df_title_word_occurrence.to_csv('Data/processed/title_word_occurrence.csv')\n",
    "#profanity Anteil\n",
    "\n",
    "#sentiment Analyse\n",
    "#Unique WÃ¶rter Anzahl Analyse\n",
    "\n",
    "#Korreltaiton zwischen Interpret und Beliebtheit:\n",
    "#df['INTERPRET'].value_counts(ascending=True, dropna=True, normalize=True)\\\n",
    "#.to_csv('Data/processed/Interpret_Occurrences_normalised.csv')\n",
    "\n",
    "#df[['LAENGE_LAENGSTES_WORT', 'Jahr','ID','RANK_SCORE']].to_csv('Data/processed/Wort_LAENGE.csv')\n",
    "#df[['ID','Number_of_Stopwords','Stopword_Percentage','Jahr', 'RANK_SCORE']].to_csv('Data/processed/Stoppwords.csv')\n",
    "\n",
    "# Titel Analyse (lÃ¤nge des Titels, WÃ¶rter des Textes, ...)\n",
    "\n",
    "df.to_csv('Data/processed/EDA3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
